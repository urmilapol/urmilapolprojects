{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RNNBINADD.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyN3N3lLaZRHxi/0aeGKHAQS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/urmilapol/urmilapolprojects/blob/master/RNNBINADD.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Recurrent Neural Network from Scratch in Python 3\n",
        "\n",
        "import copy\n",
        "import numpy as np\n",
        "\n",
        "# np.random.seed(0)\n",
        "\n",
        "# Sigmoid Activation Function\n",
        "# To be applied at Hidden Layers and Output Layer\n",
        "def sigmoid(z):\n",
        "    return (1 / (1 + np.exp(-z)))\n",
        "\n",
        "# Derivative of Sigmoid Function\n",
        "# Used in calculation of Back Propagation Loss\n",
        "def sigmoidPrime(z):\n",
        "    return z * (1-z)\n",
        "\n",
        "\n",
        "# Generate Input Dataset\n",
        "int_to_binary = {}\n",
        "binary_dim = 8\n",
        "\n",
        "# Calculate the largest value which can be attained\n",
        "# 2^8 = 256\n",
        "max_val = (2**binary_dim)\n",
        "\n",
        "# Calculate Binary values for int from 0 to 256\n",
        "binary_val = np.unpackbits(np.array([range(max_val)], dtype=np.uint8).T, axis=1)\n",
        "\n",
        "# Function to map Integer values to Binary values\n",
        "for i in range(max_val):\n",
        "    int_to_binary[i] = binary_val[i]\n",
        "    # print('\\nInteger value: ',i)\n",
        "    # print('binary value: ', binary_val[i])\n",
        "\n",
        "\n",
        "# NN variables\n",
        "learning_rate = 0.1\n",
        "\n",
        "# Inputs: Values to be added bit by bit\n",
        "inputLayerSize = 2\n",
        "\n",
        "# Hidden Layer with 16 neurons\n",
        "hiddenLayerSize = 16\n",
        "\n",
        "# Output at one time step is 1 bit\n",
        "outputLayerSize = 1\n",
        "\n",
        "# Initialize Weights\n",
        "# Weight of first Synapse (Synapse_0) from Input to Hidden Layer at Current Timestep\n",
        "W1 = 2 * np.random.random((inputLayerSize, hiddenLayerSize)) - 1\n",
        "\n",
        "# Weight of second Synapse (Synapse_1) from Hidden Layer to Output Layer\n",
        "W2 = 2 * np.random.random((hiddenLayerSize, outputLayerSize)) - 1\n",
        "\n",
        "# Weight of Synapse (Synapse_h) from Current Hidden Layer to Next Hidden Layer in Timestep\n",
        "W_h = 2 * np.random.random((hiddenLayerSize, hiddenLayerSize)) - 1\n",
        "\n",
        "\n",
        "# Initialize Updated Weights Values\n",
        "W1_update = np.zeros_like(W1)\n",
        "W2_update = np.zeros_like(W2)\n",
        "W_h_update = np.zeros_like(W_h)\n",
        "\n",
        "\n",
        "# Iterate over 10,000 samples for Training\n",
        "for j in range(10000):\n",
        "    # ----------------------------- Compute True Values for the Sum (a+b) [binary encoded] --------------------------\n",
        "    # Generate a random sample value for 1st input\n",
        "    a_int = np.random.randint(max_val/2)\n",
        "    # Convert this Int value to Binary\n",
        "    a = int_to_binary[a_int]\n",
        "\n",
        "    # Generate a random sample value for 2nd input\n",
        "    b_int = np.random.randint(max_val/2)\n",
        "    # Map Int to Binary\n",
        "    b = int_to_binary[b_int]\n",
        "\n",
        "    # True Answer a + b = c\n",
        "    c_int = a_int + b_int\n",
        "    c = int_to_binary[c_int]\n",
        "\n",
        "    # Array to save predicted outputs (binary encoded)\n",
        "    d = np.zeros_like(c)\n",
        "\n",
        "    # Initialize overall error to \"0\"\n",
        "    overallError = 0\n",
        "\n",
        "    # Save the values of dJdW1 and dJdW2 computed at Output layer into a list\n",
        "    output_layer_deltas = list()\n",
        "\n",
        "    # Save the values obtained at Hidden Layer of current state in a list to keep track\n",
        "    hidden_layer_values = list()\n",
        "\n",
        "    # Initially, there is no previous hidden state. So append \"0\" for that\n",
        "    hidden_layer_values.append(np.zeros(hiddenLayerSize))\n",
        "\n",
        "    # ----------------------------- Compute the Values for (a+b) using RNN [Forward Propagation] ----------------------\n",
        "    # position: location of the bit amongst 8 bits; starting point \"0\"; \"0 - 7\"\n",
        "    for position in range(binary_dim):\n",
        "        # Generate Input Data for RNN\n",
        "        # Take the binary values of \"a\" and \"b\" generated for each iteration of \"j\"\n",
        "\n",
        "        # With increasing value of position, the bit location of \"a\" and \"b\" decreases from \"7 -> 0\"\n",
        "        # and each iteration computes the sum of corresponding bit of \"a\" and \"b\".\n",
        "        # ex. for position = 0, X = [a[7],b[7]], 7th bit of a and b.\n",
        "        X = np.array([[a[binary_dim - position - 1], b[binary_dim - position - 1]]])\n",
        "\n",
        "        # Actual value for (a+b) = c, c is an array of 8 bits, so take transpose to compare bit by bit with X value.\n",
        "        y = np.array([[c[binary_dim - position - 1]]]).T\n",
        "\n",
        "        # Values computed at current hidden layer\n",
        "        # [dot product of Input(X) and Weights(W1)] + [dot product of previous hidden layer values and Weights (W_h)]\n",
        "        # W_h: weight from previous step hidden layer to current step hidden layer\n",
        "        # W1: weights from current step input to current hidden layer\n",
        "        layer_1 = sigmoid(np.dot(X,W1) + np.dot(hidden_layer_values[-1],W_h))\n",
        "\n",
        "        # The new output using new Hidden layer values\n",
        "        layer_2 = sigmoid(np.dot(layer_1, W2))\n",
        "\n",
        "        # Calculate the error\n",
        "        output_error = y - layer_2\n",
        "\n",
        "        # Save the error deltas at each step as it will be propagated back\n",
        "        output_layer_deltas.append((output_error)*sigmoidPrime(layer_2))\n",
        "\n",
        "        # Save the sum of error at each binary position\n",
        "        overallError += np.abs(output_error[0])\n",
        "\n",
        "        # Round off the values to nearest \"0\" or \"1\" and save it to a list\n",
        "        d[binary_dim - position - 1] = np.round(layer_2[0][0])\n",
        "\n",
        "        # Save the hidden layer to be used later\n",
        "        hidden_layer_values.append(copy.deepcopy(layer_1))\n",
        "\n",
        "    future_layer_1_delta = np.zeros(hiddenLayerSize)\n",
        "\n",
        "# ----------------------------------- Back Propagating the Error Values to All Previous Time-steps ---------------------\n",
        "    for position in range(binary_dim):\n",
        "        # a[0], b[0] -> a[1]b[1] ....\n",
        "        X = np.array([[a[position], b[position]]])\n",
        "        # The last step Hidden Layer where we are currently a[0],b[0]\n",
        "        layer_1 = hidden_layer_values[-position - 1]\n",
        "        # The hidden layer before the current layer, a[1],b[1]\n",
        "        prev_hidden_layer = hidden_layer_values[-position-2]\n",
        "        # Errors at Output Layer, a[1],b[1]\n",
        "        output_layer_delta = output_layer_deltas[-position-1]\n",
        "        layer_1_delta = (future_layer_1_delta.dot(W_h.T) + output_layer_delta.dot(W2.T)) * sigmoidPrime(layer_1)\n",
        "\n",
        "        # Update all the weights and try again\n",
        "        W2_update += np.atleast_2d(layer_1).T.dot(output_layer_delta)\n",
        "        W_h_update += np.atleast_2d(prev_hidden_layer).T.dot(layer_1_delta)\n",
        "        W1_update += X.T.dot(layer_1_delta)\n",
        "\n",
        "        future_layer_1_delta = layer_1_delta\n",
        "\n",
        "    # Update the weights with the values\n",
        "    W1 += W1_update * learning_rate\n",
        "    W2 += W2_update * learning_rate\n",
        "    W_h += W_h_update * learning_rate\n",
        "\n",
        "    # Clear the updated weights values\n",
        "    W1_update *= 0\n",
        "    W2_update *= 0\n",
        "    W_h_update *= 0\n",
        "\n",
        "\n",
        "    # Print out the Progress of the RNN\n",
        "    if (j % 1000 == 0):\n",
        "        print(\"Error:\" + str(overallError))\n",
        "        print(\"Pred:\" + str(d))\n",
        "        print(\"True:\" + str(c))\n",
        "        out = 0\n",
        "        for index, x in enumerate(reversed(d)):\n",
        "            out += x * pow(2, index)\n",
        "        print(str(a_int) + \" + \" + str(b_int) + \" = \" + str(out))\n",
        "        print(\"------------\")\n",
        "\n",
        "# ------------------------------------- EOC -----------------------------"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K8_eimmFA1SD",
        "outputId": "f68951ae-6691-46f0-9c4f-7d6674ec00f8"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error:[3.97865974]\n",
            "Pred:[0 0 0 0 0 0 0 0]\n",
            "True:[1 0 0 0 0 1 1 1]\n",
            "85 + 50 = 0\n",
            "------------\n",
            "Error:[3.84241807]\n",
            "Pred:[0 0 1 1 0 0 0 0]\n",
            "True:[0 0 0 1 1 1 1 0]\n",
            "24 + 6 = 48\n",
            "------------\n",
            "Error:[4.09324687]\n",
            "Pred:[1 1 1 1 0 0 1 0]\n",
            "True:[1 1 0 0 1 0 0 1]\n",
            "92 + 109 = 242\n",
            "------------\n",
            "Error:[3.63362897]\n",
            "Pred:[1 0 0 1 0 1 0 1]\n",
            "True:[1 0 0 1 0 1 0 1]\n",
            "116 + 33 = 149\n",
            "------------\n",
            "Error:[2.59825255]\n",
            "Pred:[1 0 1 1 1 1 0 0]\n",
            "True:[1 0 1 1 1 0 0 0]\n",
            "123 + 61 = 188\n",
            "------------\n",
            "Error:[1.05529902]\n",
            "Pred:[1 0 0 1 1 0 1 1]\n",
            "True:[1 0 0 1 1 0 1 1]\n",
            "33 + 122 = 155\n",
            "------------\n",
            "Error:[0.67683728]\n",
            "Pred:[1 0 0 1 1 1 1 0]\n",
            "True:[1 0 0 1 1 1 1 0]\n",
            "31 + 127 = 158\n",
            "------------\n",
            "Error:[0.30935664]\n",
            "Pred:[0 1 1 1 1 1 1 0]\n",
            "True:[0 1 1 1 1 1 1 0]\n",
            "84 + 42 = 126\n",
            "------------\n",
            "Error:[0.43330915]\n",
            "Pred:[0 1 0 0 1 0 1 1]\n",
            "True:[0 1 0 0 1 0 1 1]\n",
            "29 + 46 = 75\n",
            "------------\n",
            "Error:[0.40390565]\n",
            "Pred:[1 0 0 0 0 1 1 1]\n",
            "True:[1 0 0 0 0 1 1 1]\n",
            "125 + 10 = 135\n",
            "------------\n"
          ]
        }
      ]
    }
  ]
}