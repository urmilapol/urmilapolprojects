{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "T5 summarization",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/urmilapol/urmilapolprojects/blob/master/T5_summarization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0cf75iN9zge2"
      },
      "source": [
        "# Fine Tuning Transformer for Summary Generation\n",
        "\n",
        "* T5 network loaded via huggingface transformers using pytorch\n",
        "* run in colab on free gpu (batch-size <= 4 for T5-base / 128 for T5-small)\n",
        "---\n",
        "\n",
        "\n",
        "adapted from https://colab.research.google.com/github/abhimishra91/transformers-tutorials/blob/master/transformers_summarization_wandb.ipynb#scrollTo=j9TNdHlQ0CLz"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A3RD1CMHsa7c"
      },
      "source": [
        "!pip install transformers -q"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BzZXu6ohtLj3"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler\n",
        "\n",
        "# Importing the T5 modules from huggingface/transformers\n",
        "from transformers import T5Tokenizer, T5ForConditionalGeneration"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fwCb8IUGtAK7"
      },
      "source": [
        "Check GPU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G7UfC9b9te2J",
        "outputId": "6bca712d-8981-48bb-d807-c343a6194986",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        }
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Wed Oct 28 10:30:07 2020       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 455.23.05    Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   68C    P8    12W /  70W |     10MiB / 15079MiB |      0%      Default |\n",
            "|                               |                      |                 ERR! |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T_vGD3PsthLp"
      },
      "source": [
        "# # Setting up the device for GPU usage\n",
        "from torch import cuda\n",
        "device = 'cuda' if cuda.is_available() else 'cpu'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IrrFMV44vu4D"
      },
      "source": [
        "# specifiy dataset and column names\n",
        "path = './data/news_summary.csv'\n",
        "summary_col = 'text'\n",
        "fulltext_col = 'ctext'\n",
        "\n",
        "config = {\n",
        "    'TRAIN_BATCH_SIZE': 4,     # input batch size for training (default: 64)\n",
        "    'VALID_BATCH_SIZE': 4,   # input batch size for testing (default: 1000)\n",
        "    'TRAIN_EPOCHS' : 3,        # number of epochs to train (default: 10)\n",
        "    'LEARNING_RATE' : 1e-4,    # learning rate (default: 0.01)\n",
        "    'SEED' : 42,               # random seed (default: 42)\n",
        "    'MAX_LEN' : 512,\n",
        "    'SUMMARY_LEN' : 150\n",
        "    }"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "932p8NhxeNw4"
      },
      "source": [
        "# Creating a custom dataset for reading the dataframe and loading it into the dataloader to pass it to the neural network at a later stage for finetuning the model and to prepare it for predictions\n",
        "\n",
        "class CustomDataset(Dataset):\n",
        "\n",
        "    def __init__(self, dataframe, tokenizer, source_len, summ_len):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.data = dataframe\n",
        "        self.source_len = source_len\n",
        "        self.summ_len = summ_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data.summary)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        fulltext = str(self.data.fulltext[index])\n",
        "        fulltext = ' '.join(fulltext.split())\n",
        "\n",
        "        summary = str(self.data.summary[index])\n",
        "        summary = ' '.join(summary.split())\n",
        "\n",
        "        source = self.tokenizer.batch_encode_plus([fulltext], max_length= self.source_len, pad_to_max_length=True,return_tensors='pt')\n",
        "        target = self.tokenizer.batch_encode_plus([summary], max_length= self.summ_len, pad_to_max_length=True,return_tensors='pt')\n",
        "\n",
        "        source_ids = source['input_ids'].squeeze()\n",
        "        source_mask = source['attention_mask'].squeeze()\n",
        "        target_ids = target['input_ids'].squeeze()\n",
        "        target_mask = target['attention_mask'].squeeze()\n",
        "\n",
        "        return {\n",
        "            'source_ids': source_ids.to(dtype=torch.long),\n",
        "            'source_mask': source_mask.to(dtype=torch.long),\n",
        "            'target_ids': target_ids.to(dtype=torch.long),\n",
        "            'target_ids_y': target_ids.to(dtype=torch.long)\n",
        "        }"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SaPAR7TWmxoM"
      },
      "source": [
        "# Creating the training function. This will be called in the main function. It is run depending on the epoch value.\n",
        "# The model is put into train mode and then we enumerate over the training loader and passed to the defined network\n",
        "\n",
        "def train(epoch, tokenizer, model, device, loader, optimizer):\n",
        "    model.train()\n",
        "    for _,data in enumerate(loader, 0):\n",
        "        y = data['target_ids'].to(device, dtype = torch.long)\n",
        "        y_ids = y[:, :-1].contiguous()\n",
        "        lm_labels = y[:, 1:].clone().detach()\n",
        "        lm_labels[y[:, 1:] == tokenizer.pad_token_id] = -100\n",
        "        ids = data['source_ids'].to(device, dtype = torch.long)\n",
        "        mask = data['source_mask'].to(device, dtype = torch.long)\n",
        "\n",
        "        outputs = model(input_ids = ids, attention_mask = mask, decoder_input_ids=y_ids, lm_labels=lm_labels)\n",
        "        loss = outputs[0]\n",
        "\n",
        "        if _%50==0:\n",
        "            print(f'Epoch: {epoch}, Loss:  {loss.item()}')\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        # xm.optimizer_step(optimizer)\n",
        "        # xm.mark_step()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j9TNdHlQ0CLz"
      },
      "source": [
        "def validate(tokenizer, model, device, loader):\n",
        "    model.eval()\n",
        "    predictions = []\n",
        "    actuals = []\n",
        "    with torch.no_grad():\n",
        "        for _, data in enumerate(loader, 0):\n",
        "            y = data['target_ids'].to(device, dtype = torch.long)\n",
        "            ids = data['source_ids'].to(device, dtype = torch.long)\n",
        "            mask = data['source_mask'].to(device, dtype = torch.long)\n",
        "\n",
        "            generated_ids = model.generate(\n",
        "                input_ids = ids,\n",
        "                attention_mask = mask,\n",
        "                max_length=150,\n",
        "                num_beams=2,\n",
        "                repetition_penalty=2.5,\n",
        "                length_penalty=1.0,\n",
        "                early_stopping=True\n",
        "                )\n",
        "            preds = [tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=True) for g in generated_ids]\n",
        "            target = [tokenizer.decode(t, skip_special_tokens=True, clean_up_tokenization_spaces=True)for t in y]\n",
        "            if _%100==0:\n",
        "                print(f'Completed {_}')\n",
        "\n",
        "            predictions.extend(preds)\n",
        "            actuals.extend(target)\n",
        "    return predictions, actuals"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C5cc7sLztoqp"
      },
      "source": [
        "# Set random seeds and deterministic pytorch for reproducibility\n",
        "torch.manual_seed(config['SEED']) # pytorch random seed\n",
        "np.random.seed(config['SEED']) # numpy random seed\n",
        "torch.backends.cudnn.deterministic = True\n",
        "\n",
        "# tokenzier for encoding the text\n",
        "tokenizer = T5Tokenizer.from_pretrained(\"t5-base\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4361KlRv0ivb",
        "outputId": "1272e2ad-c5b2-4f1d-d0fa-a6a89b7452a8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "# Importing and Pre-Processing the domain data\n",
        "df = pd.read_csv(path ,encoding='latin-1')\n",
        "\n",
        "# Selecting the needed columns only and renaming to summary & fulltext.\n",
        "df = df[[summary_col, fulltext_col]]\n",
        "df = df.rename({summary_col: 'summary', fulltext_col: 'fulltext'}, axis='columns')\n",
        "# Adding the summarzie text in front of the text. This is to format the dataset similar to how T5 model was trained for summarization task.\n",
        "df.fulltext = 'summarize: ' + df.fulltext\n",
        "# df = df.sample(frac=0.1)\n",
        "print(df.head())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                                             summary                                           fulltext\n",
            "0  The Administration of Union Territory Daman an...  summarize: The Daman and Diu administration on...\n",
            "1  Malaika Arora slammed an Instagram user who tr...  summarize: From her special numbers to TV?appe...\n",
            "2  The Indira Gandhi Institute of Medical Science...  summarize: The Indira Gandhi Institute of Medi...\n",
            "3  Lashkar-e-Taiba's Kashmir commander Abu Dujana...  summarize: Lashkar-e-Taiba's Kashmir commander...\n",
            "4  Hotels in Maharashtra will train their staff t...  summarize: Hotels in Mumbai and other Indian c...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5lVMz8qf04u-",
        "outputId": "079a0551-57cf-4ef9-c5c9-5b384f6f8c48",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "# Creation of Dataset and Dataloader\n",
        "# Defining the train size. So 80% of the data will be used for training and the rest will be used for validation.\n",
        "train_size = 0.8\n",
        "train_dataset=df.sample(frac=train_size,random_state = config['SEED'])\n",
        "val_dataset=df.drop(train_dataset.index).reset_index(drop=True)\n",
        "train_dataset = train_dataset.reset_index(drop=True)\n",
        "\n",
        "print(\"FULL Dataset: {}\".format(df.shape))\n",
        "print(\"TRAIN Dataset: {}\".format(train_dataset.shape))\n",
        "print(\"TEST Dataset: {}\".format(val_dataset.shape))\n",
        "\n",
        "# Creating the Training and Validation dataset for further creation of Dataloader\n",
        "training_set = CustomDataset(train_dataset, tokenizer, config['MAX_LEN'], config['SUMMARY_LEN'])\n",
        "val_set = CustomDataset(val_dataset, tokenizer, config['MAX_LEN'], config['SUMMARY_LEN'])\n",
        "\n",
        "# Defining the parameters for creation of dataloaders\n",
        "train_params = {\n",
        "    'batch_size': config['TRAIN_BATCH_SIZE'],\n",
        "    'shuffle': True,\n",
        "    'num_workers': 0\n",
        "    }\n",
        "\n",
        "val_params = {\n",
        "    'batch_size': config['VALID_BATCH_SIZE'],\n",
        "    'shuffle': False,\n",
        "    'num_workers': 0\n",
        "    }\n",
        "\n",
        "# Creation of Dataloaders for testing and validation. This will be used down for training and validation stage for the model.\n",
        "training_loader = DataLoader(training_set, **train_params)\n",
        "val_loader = DataLoader(val_set, **val_params)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "FULL Dataset: (4514, 2)\n",
            "TRAIN Dataset: (3611, 2)\n",
            "TEST Dataset: (903, 2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QVl1jZ8i1Kk6",
        "outputId": "4c079e96-562e-4e5f-d0b3-30332d37dd5a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Defining the model. We are using t5-base model and added a Language model layer on top for generation of Summary.\n",
        "# Further this model is sent to device (GPU/TPU) for using the hardware.\n",
        "model = T5ForConditionalGeneration.from_pretrained(\"t5-base\")\n",
        "model = model.to(device)\n",
        "\n",
        "# Defining the optimizer that will be used to tune the weights of the network in the training session.\n",
        "optimizer = torch.optim.Adam(params =  model.parameters(), lr=config['LEARNING_RATE'])\n",
        "\n",
        "# Training loop\n",
        "print('Initiating Fine-Tuning for the model on our dataset')\n",
        "\n",
        "for epoch in range(config['TRAIN_EPOCHS']):\n",
        "  train(epoch, tokenizer, model, device, training_loader, optimizer)\n",
        "\n",
        "torch.save(model.state_dict(), './data/t5_summary_state_dict.pt')\n",
        "\n",
        "# Validation loop and saving the resulting file with predictions and acutals in a dataframe.\n",
        "# Saving the dataframe as predictions.csv\n",
        "print('Now generating summaries on our fine tuned model for the validation dataset and saving it in a dataframe')\n",
        "predictions, actuals = validate(tokenizer, model, device, val_loader)\n",
        "final_df = pd.DataFrame({'Generated Text':predictions,'Actual Text':actuals})\n",
        "final_df.to_csv('./data/predictions.csv')\n",
        "print('Output Files generated for review')\n",
        "print(final_df.head())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Initiating Fine-Tuning for the model on our dataset\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/modeling_t5.py:1146: FutureWarning: The `lm_labels` argument is deprecated and will be removed in a future version, use `labels` instead.\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 0, Loss:  5.625776767730713\n",
            "Epoch: 0, Loss:  2.256822109222412\n",
            "Epoch: 0, Loss:  2.451709508895874\n",
            "Epoch: 0, Loss:  1.7885276079177856\n",
            "Epoch: 0, Loss:  2.0137434005737305\n",
            "Epoch: 0, Loss:  1.7009224891662598\n",
            "Epoch: 0, Loss:  1.5155736207962036\n",
            "Epoch: 0, Loss:  1.5952929258346558\n",
            "Epoch: 0, Loss:  1.7566555738449097\n",
            "Epoch: 0, Loss:  1.631577730178833\n",
            "Epoch: 0, Loss:  1.605650544166565\n",
            "Epoch: 0, Loss:  1.6535598039627075\n",
            "Epoch: 0, Loss:  1.7815755605697632\n",
            "Epoch: 0, Loss:  1.117836833000183\n",
            "Epoch: 0, Loss:  1.4934539794921875\n",
            "Epoch: 0, Loss:  1.6505780220031738\n",
            "Epoch: 0, Loss:  1.305673599243164\n",
            "Epoch: 0, Loss:  1.7320071458816528\n",
            "Epoch: 0, Loss:  2.0669302940368652\n",
            "Epoch: 1, Loss:  1.7653343677520752\n",
            "Epoch: 1, Loss:  2.2454097270965576\n",
            "Epoch: 1, Loss:  1.557674765586853\n",
            "Epoch: 1, Loss:  1.857395887374878\n",
            "Epoch: 1, Loss:  1.2440931797027588\n",
            "Epoch: 1, Loss:  1.5093859434127808\n",
            "Epoch: 1, Loss:  2.156465530395508\n",
            "Epoch: 1, Loss:  1.8368101119995117\n",
            "Epoch: 1, Loss:  1.2443758249282837\n",
            "Epoch: 1, Loss:  2.071000099182129\n",
            "Epoch: 1, Loss:  1.4164073467254639\n",
            "Epoch: 1, Loss:  2.0303401947021484\n",
            "Epoch: 1, Loss:  1.551741361618042\n",
            "Epoch: 1, Loss:  1.4163755178451538\n",
            "Epoch: 1, Loss:  1.9462475776672363\n",
            "Epoch: 1, Loss:  1.587754249572754\n",
            "Epoch: 1, Loss:  1.1913737058639526\n",
            "Epoch: 1, Loss:  1.479478120803833\n",
            "Epoch: 1, Loss:  1.1345407962799072\n",
            "Epoch: 2, Loss:  2.1713674068450928\n",
            "Epoch: 2, Loss:  1.4302208423614502\n",
            "Epoch: 2, Loss:  1.3769255876541138\n",
            "Epoch: 2, Loss:  1.9053547382354736\n",
            "Epoch: 2, Loss:  1.6915695667266846\n",
            "Epoch: 2, Loss:  1.3478834629058838\n",
            "Epoch: 2, Loss:  1.1771379709243774\n",
            "Epoch: 2, Loss:  1.4474095106124878\n",
            "Epoch: 2, Loss:  1.1125282049179077\n",
            "Epoch: 2, Loss:  1.0373234748840332\n",
            "Epoch: 2, Loss:  1.4377902746200562\n",
            "Epoch: 2, Loss:  1.1859749555587769\n",
            "Epoch: 2, Loss:  1.0735173225402832\n",
            "Epoch: 2, Loss:  1.2565574645996094\n",
            "Epoch: 2, Loss:  1.3834972381591797\n",
            "Epoch: 2, Loss:  0.9191421866416931\n",
            "Epoch: 2, Loss:  1.1672827005386353\n",
            "Epoch: 2, Loss:  1.7589744329452515\n",
            "Epoch: 2, Loss:  1.2042229175567627\n",
            "Now generating summaries on our fine tuned model for the validation dataset and saving it in a dataframe\n",
            "Completed 0\n",
            "Completed 100\n",
            "Completed 200\n",
            "Output Files generated for review\n",
            "                                      Generated Text                                        Actual Text\n",
            "0  Mumbai and other Indian cities are planning to...  Hotels in Maharashtra will train their staff t...\n",
            "1  UP Congress Party has opened a 'State Bank of ...  The Congress party has opened a bank called 'S...\n",
            "2  a 24-year-old Indian athlete has been indicted...  Tanveer Hussain, a 24-year-old Indian athlete ...\n",
            "3  the remains of a German hiker who disappeared ...  The remains of a German hiker, who disappeared...\n",
            "4  GP Manish Shah, who practised in east London, ...  A UK-based doctor, Manish Shah, has been charg...\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}