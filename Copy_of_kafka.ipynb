{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM/abvpq5DAhRJLd10nwTaD",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/urmilapol/urmilapolprojects/blob/master/Copy_of_kafka.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Real-Time Data Streaming**\n",
        "\n",
        "Adding Apache Kafka to this lab transforms it from a simple batch ETL process into a modern Event-Driven Data Pipeline. In this scenario, we move away from \"pulling\" data and toward \"streaming\" data as it happens.\n",
        "\n",
        "The Streaming Architecture\n",
        "In a real-world data engineering environment:\n",
        "\n",
        "Producer: A web app sends user events to a Kafka Topic.\n",
        "\n",
        "Stream Processor: A Python script listens to the topic, cleans the data in real-time.\n",
        "\n",
        "Sink (Storage): The processed data is written to PostgreSQL for immediate dashboarding."
      ],
      "metadata": {
        "id": "9P7fhhlHCVR2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Install Postgres\n",
        "!apt install postgresql postgresql-contrib &> /dev/null\n",
        "\n",
        "# Configure PostgreSQL to accept password authentication for user 'postgres' and create database\n",
        "!service postgresql start\n",
        "!sudo -u postgres psql -c \"ALTER USER postgres WITH PASSWORD 'password123';\"\n",
        "!sudo -u postgres psql -c \"CREATE DATABASE silver_events;\"\n",
        "!service postgresql restart"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SiC6VrTl7YUk",
        "outputId": "d9e3941f-3d11-4cc4-8b77-6adb2b5a6993"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " * Starting PostgreSQL 14 database server\n",
            "   ...done.\n",
            "ALTER ROLE\n",
            "ERROR:  database \"silver_events\" already exists\n",
            " * Restarting PostgreSQL 14 database server\n",
            "   ...done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Install Kafka (Downloads the binaries)\n",
        "!curl -sSOL https://archive.apache.org/dist/kafka/3.1.0/kafka_2.13-3.1.0.tgz\n",
        "!tar -xzf kafka_2.13-3.1.0.tgz\n",
        "\n",
        "# 3. Start Zookeeper and Kafka Server\n",
        "!./kafka_2.13-3.1.0/bin/zookeeper-server-start.sh ./kafka_2.13-3.1.0/config/zookeeper.properties > zookeeper.log 2>&1 &\n",
        "!./kafka_2.13-3.1.0/bin/kafka-server-start.sh ./kafka_2.13-3.1.0/config/server.properties > kafka.log 2>&1 &\n",
        "\n",
        "# Give Kafka a moment to start up\n",
        "!sleep 10"
      ],
      "metadata": {
        "id": "8-qbdrsq7mpp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **This script simulates a continuous stream of user clickstream data.**"
      ],
      "metadata": {
        "id": "DcUV-1T4EDqF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import threading\n",
        "import json\n",
        "import time\n",
        "import random\n",
        "from kafka import KafkaProducer # Moved this import here\n",
        "\n",
        "!pip install kafka-python # Moved installation to the top level\n",
        "\n",
        "def run_producer():\n",
        "    # The entire producer logic will now be correctly placed and indented here\n",
        "    producer = KafkaProducer(\n",
        "        bootstrap_servers=['localhost:9092'],\n",
        "        value_serializer=lambda x: json.dumps(x).encode('utf-8')\n",
        "    )\n",
        "\n",
        "    actions = ['click', 'view', 'add_to_cart', 'purchase']\n",
        "\n",
        "    print(\"Streaming started from producer thread...\")\n",
        "    while True:\n",
        "        data = {\n",
        "            \"user_id\": random.randint(1000, 9999),\n",
        "            \"action\": random.choice(actions),\n",
        "            \"timestamp\": time.time()\n",
        "        }\n",
        "        producer.send('user-events', value=data)\n",
        "        time.sleep(1)  # Send one event per second\n",
        "\n",
        "# This starts the producer in the background\n",
        "producer_thread = threading.Thread(target=run_producer, daemon=True)\n",
        "producer_thread.start()\n",
        "print(\"Producer is running in the background...\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "642c581d-364a-4502-eba7-48a8288ed4f7",
        "id": "HBrp3bR9IjoA"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: kafka-python in /usr/local/lib/python3.12/dist-packages (2.3.0)\n",
            "Producer is running in the background...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# This script acts as the Data Engineer's \"Processor.\" It consumes the raw stream, filters out only the \"purchase\" events, and retrieves them into PostgreSQL.\n"
      ],
      "metadata": {
        "id": "DOWyzknXHqYA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from kafka import KafkaConsumer\n",
        "import psycopg2\n",
        "import json\n",
        "\n",
        "# Connect to Postgres\n",
        "conn = psycopg2.connect(\"host=localhost dbname=silver_events user=postgres password=password123\")\n",
        "cur = conn.cursor()\n",
        "\n",
        "# Create the table if it doesn't exist\n",
        "cur.execute(\"CREATE TABLE IF NOT EXISTS silver_events (u_id INTEGER, act VARCHAR(255), val INTEGER);\")\n",
        "conn.commit()\n",
        "\n",
        "# Initialize Kafka Consumer\n",
        "consumer = KafkaConsumer(\n",
        "    'user-events',\n",
        "    bootstrap_servers=['localhost:9092'],\n",
        "    value_deserializer=lambda x: json.loads(x.decode('utf-8'))\n",
        ")\n",
        "\n",
        "print(\"Listening for events...\")\n",
        "for message in consumer:\n",
        "    event = message.value\n",
        "\n",
        "    # Logic: Only store high-value events (Purchases)\n",
        "    if event['action'] == 'purchase':\n",
        "        print(f\"Storing Purchase: {event}\")\n",
        "        cur.execute(\n",
        "            \"INSERT INTO silver_events (u_id, act, val) VALUES (%s, %s, %s)\",\n",
        "            (event['user_id'], event['action'], 1)\n",
        "        )\n",
        "        conn.commit()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dkVweu2iFyZu",
        "outputId": "202b93e3-ce7d-4983-8021-23a9f1abc532"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:kafka.coordinator.consumer:group_id is None: disabling auto-commit.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Listening for events...\n",
            "Storing Purchase: {'user_id': 7044, 'action': 'purchase', 'timestamp': 1767784121.754121}\n",
            "Storing Purchase: {'user_id': 4352, 'action': 'purchase', 'timestamp': 1767784131.758446}\n",
            "Storing Purchase: {'user_id': 5594, 'action': 'purchase', 'timestamp': 1767784135.7601395}\n",
            "Storing Purchase: {'user_id': 1404, 'action': 'purchase', 'timestamp': 1767784140.7622867}\n",
            "Storing Purchase: {'user_id': 5848, 'action': 'purchase', 'timestamp': 1767784141.7627509}\n",
            "Storing Purchase: {'user_id': 8297, 'action': 'purchase', 'timestamp': 1767784145.7642791}\n",
            "Storing Purchase: {'user_id': 7883, 'action': 'purchase', 'timestamp': 1767784146.7646616}\n",
            "Storing Purchase: {'user_id': 8405, 'action': 'purchase', 'timestamp': 1767784148.7655537}\n",
            "Storing Purchase: {'user_id': 6585, 'action': 'purchase', 'timestamp': 1767784151.7667046}\n",
            "Storing Purchase: {'user_id': 6901, 'action': 'purchase', 'timestamp': 1767784152.767161}\n",
            "Storing Purchase: {'user_id': 5277, 'action': 'purchase', 'timestamp': 1767784154.7680097}\n",
            "Storing Purchase: {'user_id': 7204, 'action': 'purchase', 'timestamp': 1767784168.7740605}\n",
            "Storing Purchase: {'user_id': 5789, 'action': 'purchase', 'timestamp': 1767784170.7748234}\n",
            "Storing Purchase: {'user_id': 8571, 'action': 'purchase', 'timestamp': 1767784172.7756646}\n",
            "Storing Purchase: {'user_id': 6903, 'action': 'purchase', 'timestamp': 1767784180.7793446}\n",
            "Storing Purchase: {'user_id': 3905, 'action': 'purchase', 'timestamp': 1767784186.7816947}\n"
          ]
        }
      ]
    }
  ]
}