{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/urmilapol/urmilapolprojects/blob/master/SRRNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1dKy26EdC8LN"
      },
      "source": [
        "# Recurrent Neural Network\n",
        "\n",
        "This is a pure numpy implementation of word generation using an RNN\n",
        "\n",
        "![alt text](http://corochann.com/wp-content/uploads/2017/05/text_sequence_predict.png \"Logo Title Text 1\")\n",
        "\n",
        "We're going to have our network learn how to predict the next words in a given paragraph. This will require a recurrent architecture since the network will have to remember a sequence of characters. The order matters. 1000 iterations and we'll have pronouncable english. The longer the training time the better. You can feed it any text sequence (words, python, HTML, etc.)\n",
        "\n",
        "## What is a Recurrent Network?\n",
        "\n",
        "Feedforward networks are great for learning a pattern between a set of inputs and outputs.\n",
        "![alt text](https://www.researchgate.net/profile/Sajad_Jafari3/publication/275334508/figure/fig1/AS:294618722783233@1447253985297/Fig-1-Schematic-of-the-multilayer-feed-forward-neural-network-proposed-to-model-the.png \"Logo Title Text 1\")\n",
        "\n",
        "![alt text](https://s-media-cache-ak0.pinimg.com/236x/10/29/a9/1029a9a0534a768b4c4c2b5341bdd003--city-year-math-patterns.jpg \"Logo Title Text 1\")\n",
        "\n",
        "![alt text](https://www.researchgate.net/profile/Hamza_Guellue/publication/223079746/figure/fig5/AS:305255788105731@1449790059371/Fig-5-Configuration-of-a-three-layered-feed-forward-neural-network.png\n",
        " \"Logo Title Text 1\")\n",
        "\n",
        "- temperature & location\n",
        "- height & weight\n",
        "- car speed and brand\n",
        "\n",
        "But what if the ordering of the data matters? \n",
        "\n",
        "![alt text](http://www.aboutcurrency.com/images/university/fxvideocourse/google_chart.jpg \"Logo Title Text 1\")\n",
        "\n",
        "![alt text](http://news.mit.edu/sites/mit.edu.newsoffice/files/styles/news_article_image_top_slideshow/public/images/2016/vondrick-machine-learning-behavior-algorithm-mit-csail_0.jpg?itok=ruGmLJm2 \"Logo Title Text 1\")\n",
        "\n",
        "Alphabet, Lyrics of a song. These are stored using Conditional Memory. You can only access an element if you have access to the previous elements (like a linkedlist). \n",
        "\n",
        "Enter recurrent networks\n",
        "\n",
        "We feed the hidden state from the previous time step back into the the network at the next time step.\n",
        "\n",
        "![alt text](https://iamtrask.github.io/img/basic_recurrence_singleton.png \"Logo Title Text 1\")\n",
        "\n",
        "So instead of the data flow operation happening like this\n",
        "\n",
        "## input -> hidden -> output\n",
        "\n",
        "it happens like this\n",
        "\n",
        "## (input + prev_hidden) -> hidden -> output\n",
        "\n",
        "wait. Why not this?\n",
        "\n",
        "## (input + prev_input) -> hidden -> output\n",
        "\n",
        "Hidden recurrence learns what to remember whereas input recurrence is hard wired to just remember the immediately previous datapoint\n",
        "\n",
        "![alt text](https://image.slidesharecdn.com/ferret-rnn-151211092908/95/recurrent-neural-networks-part-1-theory-10-638.jpg?cb=1449826311 \"Logo Title Text 1\")\n",
        "\n",
        "![alt text](https://www.mathworks.com/help/examples/nnet/win64/RefLayRecNetExample_01.png \"Logo Title Text 1\")\n",
        "\n",
        "RNN Formula\n",
        "![alt text](https://cdn-images-1.medium.com/max/1440/0*TUFnE2arCrMrCvxH.png \"Logo Title Text 1\")\n",
        "\n",
        "It basically says the current hidden state h(t) is a function f of the previous hidden state h(t-1) and the current input x(t). The theta are the parameters of the function f. The network typically learns to use h(t) as a kind of lossy summary of the task-relevant aspects of the past sequence of inputs up to t.\n",
        "\n",
        "Loss function\n",
        "\n",
        "![alt text](https://cdn-images-1.medium.com/max/1440/0*ZsEG2aWfgqtk9Qk5. \"Logo Title Text 1\")\n",
        "\n",
        "The total loss for a given sequence of x values paired with a sequence of y values would then be just the sum of the losses over all the time steps. For example, if L(t) is the negative log-likelihood\n",
        "of y (t) given x (1), . . . , x (t) , then sum them up you get the loss for the sequence \n",
        "\n",
        "\n",
        "## Our steps\n",
        "\n",
        "- Initialize weights randomly\n",
        "- Give the model a char pair (input char & target char. The target char is the char the network should guess, its the next char in our sequence)\n",
        "- Forward pass (We calculate the probability for every possible next char according to the state of the model, using the paramters)\n",
        "- Measure error (the distance between the previous probability and the target char)\n",
        "- We calculate gradients for each of our parameters to see their impact they have on the loss (backpropagation through time)\n",
        "- update all parameters in the direction via gradients that help to minimise the loss\n",
        "- Repeat! Until our loss is small AF\n",
        "\n",
        "## What are some use cases?\n",
        "\n",
        "- Time series prediction (weather forecasting, stock prices, traffic volume, etc. )\n",
        "- Sequential data generation (music, video, audio, etc.)\n",
        "\n",
        "## Other Examples\n",
        "\n",
        "-https://github.com/anujdutt9/RecurrentNeuralNetwork (binary addition)\n",
        "\n",
        "## What's next? \n",
        "\n",
        "1 LSTM Networks\n",
        "2 Bidirectional networks\n",
        "3 recursive networks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jhP-GPbBC8Lf"
      },
      "source": [
        "## The code contains 4 parts\n",
        "* Load the trainning data\n",
        "  * encode char into vectors\n",
        "* Define the Recurrent Network\n",
        "* Define a loss function\n",
        "  * Forward pass\n",
        "  * Loss\n",
        "  * Backward pass\n",
        "* Define a function to create sentences from the model\n",
        "* Train the network\n",
        "  * Feed the network\n",
        "  * Calculate gradient and update the model parameters\n",
        "  * Output a text to see the progress of the training\n",
        " \n",
        "\n",
        "## Load the training data\n",
        "\n",
        "The network need a big txt file as an input.\n",
        "\n",
        "The content of the file will be used to train the network.\n",
        "\n",
        "I use Methamorphosis from Kafka (Public Domain). Because Kafka was one weird dude. I like."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Bn8ENwDqC8Li",
        "outputId": "47d1c08e-b833-4da6-c44b-01727d0b7de7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "data has 137628 chars, 80 unique\n"
          ]
        }
      ],
      "source": [
        "data = open('/content/sample_data/kafka.txt', 'r').read()\n",
        "\n",
        "chars = list(set(data)) \n",
        "data_size, vocab_size = len(data), len(chars)\n",
        "print ('data has %d chars, %d unique' % (data_size, vocab_size))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vsZHChD1C8Lm"
      },
      "source": [
        "### Encode/Decode char/vector\n",
        "\n",
        "Neural networks operate on vectors (a vector is an array of float)\n",
        "So we need a way to encode and decode a char as a vector.\n",
        "\n",
        "We'll count the number of unique chars (*vocab_size*). That will be the size of the vector. \n",
        "The vector contains only zero exept for the position of the char wherae the value is 1.\n",
        "\n",
        "#### So First let's calculate the *vocab_size*:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "scrolled": true,
        "id": "HDpVLTN2C8Ln",
        "outputId": "a3a9df58-9fc4-4a9b-b516-1c1d844b14d8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'R': 0, 'n': 1, 'Y': 2, ')': 3, 'A': 4, 'v': 5, 'M': 6, ' ': 7, 'o': 8, 'S': 9, '1': 10, 'C': 11, 'J': 12, 'i': 13, 'H': 14, '6': 15, 's': 16, '4': 17, '.': 18, 'B': 19, 'U': 20, 'K': 21, 'f': 22, 'Q': 23, 'q': 24, '(': 25, 'T': 26, '*': 27, '?': 28, 'h': 29, \"'\": 30, 'p': 31, 'D': 32, '\\n': 33, 'I': 34, 'l': 35, 'P': 36, '-': 37, '7': 38, '3': 39, 'x': 40, ';': 41, '!': 42, ',': 43, 'y': 44, 'V': 45, 'w': 46, 't': 47, 'c': 48, 'z': 49, '/': 50, 'F': 51, 'k': 52, 'e': 53, 'j': 54, '$': 55, 'g': 56, '9': 57, '\"': 58, 'E': 59, 'W': 60, 'รง': 61, 'L': 62, 'r': 63, '8': 64, '%': 65, '5': 66, '@': 67, 'N': 68, '0': 69, 'u': 70, 'm': 71, 'b': 72, 'a': 73, 'G': 74, 'X': 75, '2': 76, 'd': 77, 'O': 78, ':': 79}\n",
            "{0: 'R', 1: 'n', 2: 'Y', 3: ')', 4: 'A', 5: 'v', 6: 'M', 7: ' ', 8: 'o', 9: 'S', 10: '1', 11: 'C', 12: 'J', 13: 'i', 14: 'H', 15: '6', 16: 's', 17: '4', 18: '.', 19: 'B', 20: 'U', 21: 'K', 22: 'f', 23: 'Q', 24: 'q', 25: '(', 26: 'T', 27: '*', 28: '?', 29: 'h', 30: \"'\", 31: 'p', 32: 'D', 33: '\\n', 34: 'I', 35: 'l', 36: 'P', 37: '-', 38: '7', 39: '3', 40: 'x', 41: ';', 42: '!', 43: ',', 44: 'y', 45: 'V', 46: 'w', 47: 't', 48: 'c', 49: 'z', 50: '/', 51: 'F', 52: 'k', 53: 'e', 54: 'j', 55: '$', 56: 'g', 57: '9', 58: '\"', 59: 'E', 60: 'W', 61: 'รง', 62: 'L', 63: 'r', 64: '8', 65: '%', 66: '5', 67: '@', 68: 'N', 69: '0', 70: 'u', 71: 'm', 72: 'b', 73: 'a', 74: 'G', 75: 'X', 76: '2', 77: 'd', 78: 'O', 79: ':'}\n"
          ]
        }
      ],
      "source": [
        "char_to_ix = { ch:i for i,ch in enumerate(chars)}\n",
        "ix_to_char = { i:ch for i, ch in enumerate(chars)}\n",
        "print (char_to_ix)\n",
        "print (ix_to_char)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nViqet-kC8Lp"
      },
      "source": [
        "#### Then we create 2 dictionary to encode and decode a char to an int"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "TCzkfQ2IC8Lr"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iqL24zsEC8Lt"
      },
      "source": [
        "#### Finaly we create a vector from a char like this:\n",
        "The dictionary defined above allosw us to create a vector of size 61 instead of 256.  \n",
        "Here and exemple of the char 'a'  \n",
        "The vector contains only zeros, except at position char_to_ix['a'] where we put a 1."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "OZwR1ZWVC8Lv",
        "outputId": "8706e736-115c-4b62-e36c-7c850ac8dfc6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 1. 0. 0. 0. 0. 0. 0.]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "vector_for_char_a = np.zeros((vocab_size, 1))\n",
        "vector_for_char_a[char_to_ix['a']] = 1\n",
        "print (vector_for_char_a.ravel())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mYSlbGCWC8Lx"
      },
      "source": [
        "## Definition of the network\n",
        "\n",
        "The neural network is made of 3 layers:\n",
        "* an input layer\n",
        "* an hidden layer\n",
        "* an output layer\n",
        "\n",
        "All layers are fully connected to the next one: each node of a layer are conected to all nodes of the next layer.\n",
        "The hidden layer is connected to the output and to itself: the values from an iteration are used for the next one.\n",
        "\n",
        "To centralise values that matter for the training (_hyper parameters_) we also define the _sequence lenght_ and the _learning rate_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "UCOLZJnsC8Ly"
      },
      "outputs": [],
      "source": [
        "#hyperparameters\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "T_UYGRbVC8L0"
      },
      "outputs": [],
      "source": [
        "#model parameters\n",
        "\n",
        "hidden_size = 100\n",
        "seq_length = 25\n",
        "learning_rate = 1e-1\n",
        "\n",
        "Wxh = np.random.randn(hidden_size, vocab_size) * 0.01 #input to hidden\n",
        "Whh = np.random.randn(hidden_size, hidden_size) * 0.01 #input to hidden\n",
        "Why = np.random.randn(vocab_size, hidden_size) * 0.01 #input to hidden\n",
        "bh = np.zeros((hidden_size, 1))\n",
        "by = np.zeros((vocab_size, 1))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dGypUsc6C8L2"
      },
      "source": [
        "The model parameters are adjusted during the trainning.\n",
        "* _Wxh_ are parameters to connect a vector that contain one input to the hidden layer.\n",
        "* _Whh_ are parameters to connect the hidden layer to itself. This is the Key of the Rnn: Recursion is done by injecting the previous values from the output of the hidden state, to itself at the next iteration.\n",
        "* _Why_ are parameters to connect the hidden layer to the output\n",
        "* _bh_ contains the hidden bias\n",
        "* _by_ contains the output bias\n",
        "\n",
        "You'll see in the next section how theses parameters are used to create a sentence."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qoh0ThnAC8L2"
      },
      "source": [
        "## Define the loss function\n",
        "\n",
        "The __loss__ is a key concept in all neural networks training. \n",
        "It is a value that describe how good is our model.  \n",
        "The smaller the loss, the better our model is.  \n",
        "(A good model is a model where the predicted output is close to the training output)\n",
        "  \n",
        "During the training phase we want to minimize the loss.\n",
        "\n",
        "The loss function calculates the loss but also the gradients (see backward pass):\n",
        "* It perform a forward pass: calculate the next char given a char from the training set.\n",
        "* It calculate the loss by comparing the predicted char to the target char. (The target char is the input following char in the tranning set)\n",
        "* It calculate the backward pass to calculate the gradients \n",
        "\n",
        "This function take as input:\n",
        "* a list of input char\n",
        "* a list of target char\n",
        "* and the previous hidden state\n",
        "\n",
        "This function outputs:\n",
        "* the loss\n",
        "* the gradient for each parameters between layers\n",
        "* the last hidden state\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YRDXKG3zC8L4"
      },
      "source": [
        "### Forward pass\n",
        "The forward pass use the parameters of the model (Wxh, Whh, Why, bh, by) to calculate the next char given a char from the trainning set.\n",
        "\n",
        "xs[t] is the vector that encode the char at position t\n",
        "ps[t] is the probabilities for next char\n",
        "\n",
        "![alt text](https://deeplearning4j.org/img/recurrent_equation.png \"Logo Title Text 1\")\n",
        "\n",
        "```python\n",
        "hs[t] = np.tanh(np.dot(Wxh, xs[t]) + np.dot(Whh, hs[t-1]) + bh) # hidden state\n",
        "ys[t] = np.dot(Why, hs[t]) + by # unnormalized log probabilities for next chars\n",
        "ps[t] = np.exp(ys[t]) / np.sum(np.exp(ys[t])) # probabilities for next chars\n",
        "```\n",
        "\n",
        "or is dirty pseudo code for each char\n",
        "```python\n",
        "hs = input*Wxh + last_value_of_hidden_state*Whh + bh\n",
        "ys = hs*Why + by\n",
        "ps = normalized(ys)\n",
        "```\n",
        "\n",
        "### Backward pass\n",
        "\n",
        "The naive way to calculate all gradients would be to recalculate a loss for small variations for each parameters.\n",
        "This is possible but would be time consuming.\n",
        "There is a technics to calculates all the gradients for all the parameters at once: the backdrop propagation.  \n",
        "Gradients are calculated in the oposite order of the forward pass, using simple technics.  \n",
        "\n",
        "#### goal is to calculate gradients for the forward formula:\n",
        "```python\n",
        "hs = input*Wxh + last_value_of_hidden_state*Whh + bh  \n",
        "ys = hs*Why + by\n",
        "```\n",
        "\n",
        "The loss for one datapoint\n",
        "![alt text](http://i.imgur.com/LlIMvek.png \"Logo Title Text 1\")\n",
        "\n",
        "How should the computed scores inside f change tto decrease the loss? We'll need to derive a gradient to figure that out.\n",
        "\n",
        "Since all output units contribute to the error of each hidden unit we sum up all the gradients calculated at each time step in the sequence and use it to update the parameters. So our parameter gradients becomes :\n",
        "\n",
        "![alt text](http://i.imgur.com/Ig9WGqP.png \"Logo Title Text 1\")\n",
        "\n",
        "Our first gradient of our loss. We'll backpropagate this via chain rule\n",
        "\n",
        "![alt text](http://i.imgur.com/SOJcNLg.png \"Logo Title Text 1\")\n",
        "\n",
        "The chain rule is a method for finding the derivative of composite functions, or functions that are made by combining one or more functions.\n",
        "\n",
        "![alt text](http://i.imgur.com/3Z2Rfdi.png \"Logo Title Text 1\")\n",
        "\n",
        "![alt text](http://mathpullzone-8231.kxcdn.com/wp-content/uploads/thechainrule-image3.jpg \"Logo Title Text 1\")\n",
        "\n",
        "![alt text](https://i0.wp.com/www.mathbootcamps.com/wp-content/uploads/thechainrule-image1.jpg?w=900 \"Logo Title Text 1\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "wPFLy-22C8L5"
      },
      "outputs": [],
      "source": [
        "\n",
        "def lossFun(inputs, targets, hprev):\n",
        "  \"\"\"                                                                                                                                                                                         \n",
        "  inputs,targets are both list of integers.                                                                                                                                                   \n",
        "  hprev is Hx1 array of initial hidden state                                                                                                                                                  \n",
        "  returns the loss, gradients on model parameters, and last hidden state                                                                                                                      \n",
        "  \"\"\"\n",
        "  #store our inputs, hidden states, outputs, and probability values\n",
        "  xs, hs, ys, ps, = {}, {}, {}, {} #Empty dicts\n",
        "    # Each of these are going to be SEQ_LENGTH(Here 25) long dicts i.e. 1 vector per time(seq) step\n",
        "    # xs will store 1 hot encoded input characters for each of 25 time steps (26, 25 times)\n",
        "    # hs will store hidden state outputs for 25 time steps (100, 25 times)) plus a -1 indexed initial state\n",
        "    # to calculate the hidden state at t = 0\n",
        "    # ys will store targets i.e. expected outputs for 25 times (26, 25 times), unnormalized probabs\n",
        "    # ps will take the ys and convert them to normalized probab for chars\n",
        "    # We could have used lists BUT we need an entry with -1 to calc the 0th hidden layer\n",
        "    # -1 as  a list index would wrap around to the final element\n",
        "  xs, hs, ys, ps = {}, {}, {}, {}\n",
        "  #init with previous hidden state\n",
        "    # Using \"=\" would create a reference, this creates a whole separate copy\n",
        "    # We don't want hs[-1] to automatically change if hprev is changed\n",
        "  hs[-1] = np.copy(hprev)\n",
        "  #init loss as 0\n",
        "  loss = 0\n",
        "  # forward pass                                                                                                                                                                              \n",
        "  for t in xrange(len(inputs)):\n",
        "    xs[t] = np.zeros((vocab_size,1)) # encode in 1-of-k representation (we place a 0 vector as the t-th input)                                                                                                                     \n",
        "    xs[t][inputs[t]] = 1 # Inside that t-th input we use the integer in \"inputs\" list to  set the correct\n",
        "    hs[t] = np.tanh(np.dot(Wxh, xs[t]) + np.dot(Whh, hs[t-1]) + bh) # hidden state                                                                                                            \n",
        "    ys[t] = np.dot(Why, hs[t]) + by # unnormalized log probabilities for next chars                                                                                                           \n",
        "    ps[t] = np.exp(ys[t]) / np.sum(np.exp(ys[t])) # probabilities for next chars                                                                                                              \n",
        "    loss += -np.log(ps[t][targets[t],0]) # softmax (cross-entropy loss)                                                                                                                       \n",
        "  # backward pass: compute gradients going backwards    \n",
        "  #initalize vectors for gradient values for each set of weights \n",
        "  dWxh, dWhh, dWhy = np.zeros_like(Wxh), np.zeros_like(Whh), np.zeros_like(Why)\n",
        "  dbh, dby = np.zeros_like(bh), np.zeros_like(by)\n",
        "  dhnext = np.zeros_like(hs[0])\n",
        "  for t in reversed(xrange(len(inputs))):\n",
        "    #output probabilities\n",
        "    dy = np.copy(ps[t])\n",
        "    #derive our first gradient\n",
        "    dy[targets[t]] -= 1 # backprop into y  \n",
        "    #compute output gradient -  output times hidden states transpose\n",
        "    #When we apply the transpose weight matrix,  \n",
        "    #we can think intuitively of this as moving the error backward\n",
        "    #through the network, giving us some sort of measure of the error \n",
        "    #at the output of the lth layer. \n",
        "    #output gradient\n",
        "    dWhy += np.dot(dy, hs[t].T)\n",
        "    #derivative of output bias\n",
        "    dby += dy\n",
        "    #backpropagate!\n",
        "    dh = np.dot(Why.T, dy) + dhnext # backprop into h                                                                                                                                         \n",
        "    dhraw = (1 - hs[t] * hs[t]) * dh # backprop through tanh nonlinearity                                                                                                                     \n",
        "    dbh += dhraw #derivative of hidden bias\n",
        "    dWxh += np.dot(dhraw, xs[t].T) #derivative of input to hidden layer weight\n",
        "    dWhh += np.dot(dhraw, hs[t-1].T) #derivative of hidden layer to hidden layer weight\n",
        "    dhnext = np.dot(Whh.T, dhraw) \n",
        "  for dparam in [dWxh, dWhh, dWhy, dbh, dby]:\n",
        "    np.clip(dparam, -5, 5, out=dparam) # clip to mitigate exploding gradients                                                                                                                 \n",
        "  return loss, dWxh, dWhh, dWhy, dbh, dby, hs[len(inputs)-1]\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ulirLIRpC8L6"
      },
      "source": [
        "## Create a sentence from the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "qyVAyUy8C8L6",
        "outputId": "18a6ab4c-d22f-4fb0-d2ef-52296638f3ef",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----\n",
            " zXrรงvxXg,86qmpO-bfmF6HYK:H?b@JVr.Uu!HEr(D!wd:mS!\"wwn;jiPzP\n",
            "xq8l0xB?':jb:w!@LeP?WV!\n",
            "A(n6R7aVK%4wy\n",
            "2)P5(V!kq;C0tWtoK-mhCXtU.y(2EAgPo90F?xoP*Fyk5 :Wi69@?mL'@qjR3vT0;hBG;uVjF/qpvezgxXwHi0tAc0iwDO$Ec?BjFn? \n",
            "----\n"
          ]
        }
      ],
      "source": [
        "#prediction, one full forward pass\n",
        "from past.builtins import xrange\n",
        "def sample(h, seed_ix, n):\n",
        "  \"\"\"                                                                                                                                                                                         \n",
        "  sample a sequence of integers from the model                                                                                                                                                \n",
        "  h is memory state, seed_ix is seed letter for first time step   \n",
        "  n is how many characters to predict\n",
        "  \"\"\"\n",
        "  #create vector\n",
        "  x = np.zeros((vocab_size, 1))\n",
        "  #customize it for our seed char\n",
        "  x[seed_ix] = 1\n",
        "  #list to store generated chars\n",
        "  ixes = []\n",
        "  #for as many characters as we want to generate\n",
        "  for t in xrange(n):\n",
        "    #a hidden state at a given time step is a function \n",
        "    #of the input at the same time step modified by a weight matrix \n",
        "    #added to the hidden state of the previous time step \n",
        "    #multiplied by its own hidden state to hidden state matrix.\n",
        "    h = np.tanh(np.dot(Wxh, x) + np.dot(Whh, h) + bh)\n",
        "    #compute output (unnormalised)\n",
        "    y = np.dot(Why, h) + by\n",
        "    ## probabilities for next chars\n",
        "    p = np.exp(y) / np.sum(np.exp(y))\n",
        "    #pick one with the highest probability \n",
        "    ix = np.random.choice(range(vocab_size), p=p.ravel())\n",
        "    #create a vector\n",
        "    x = np.zeros((vocab_size, 1))\n",
        "    #customize it for the predicted char\n",
        "    x[ix] = 1\n",
        "    #add it to the list\n",
        "    ixes.append(ix)\n",
        "\n",
        "  txt = ''.join(ix_to_char[ix] for ix in ixes)\n",
        "  print ('----\\n %s \\n----' % (txt, ))\n",
        "hprev = np.zeros((hidden_size,1)) # reset RNN memory  \n",
        "#predict the 200 next characters given 'a'\n",
        "sample(hprev,char_to_ix['a'],200)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MBUPRbVjC8L7"
      },
      "source": [
        "\n",
        "## Training\n",
        "\n",
        "This last part of the code is the main trainning loop:\n",
        "* Feed the network with portion of the file. Size of chunk is *seq_lengh*\n",
        "* Use the loss function to:\n",
        "  * Do forward pass to calculate all parameters for the model for a given input/output pairs\n",
        "  * Do backward pass to calculate all gradiens\n",
        "* Print a sentence from a random seed using the parameters of the network\n",
        "* Update the model using the Adaptative Gradien technique Adagrad"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "qzknhBJmC8L7"
      },
      "source": [
        "### Feed the loss function with inputs and targets\n",
        "\n",
        "We create two array of char from the data file,\n",
        "the targets one is shifted compare to the inputs one.\n",
        "\n",
        "For each char in the input array, the target array give the char that follows."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "SLfuOZniC8L8",
        "outputId": "59f4efca-eab6-460e-a0a1-b61eed6dd62d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "inputs [78, 1, 53, 7, 71, 8, 63, 1, 13, 1, 56, 43, 7, 46, 29, 53, 1, 7, 74, 63, 53, 56, 8, 63, 7]\n",
            "targets [1, 53, 7, 71, 8, 63, 1, 13, 1, 56, 43, 7, 46, 29, 53, 1, 7, 74, 63, 53, 56, 8, 63, 7, 9]\n"
          ]
        }
      ],
      "source": [
        "p=0  \n",
        "inputs = [char_to_ix[ch] for ch in data[p:p+seq_length]]\n",
        "print (\"inputs\", inputs)\n",
        "targets = [char_to_ix[ch] for ch in data[p+1:p+seq_length+1]]\n",
        "print (\"targets\", targets)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gnmO1h6NC8L9"
      },
      "source": [
        "### Adagrad to update the parameters\n",
        "\n",
        "This is a type of gradient descent strategy\n",
        "\n",
        "![alt text](http://www.logos.t.u-tokyo.ac.jp/~hassy/deep_learning/adagrad/adagrad2.png\n",
        " \"Logo Title Text 1\")\n",
        "\n",
        "\n",
        "\n",
        "step size = learning rate\n",
        "\n",
        "The easiest technics to update the parmeters of the model is this:\n",
        "\n",
        "```python\n",
        "param += dparam * step_size\n",
        "```\n",
        "Adagrad is a more efficient technique where the step_size are getting smaller during the training.\n",
        "\n",
        "It use a memory variable that grow over time:\n",
        "```python\n",
        "mem += dparam * dparam\n",
        "```\n",
        "and use it to calculate the step_size:\n",
        "```python\n",
        "step_size = 1./np.sqrt(mem + 1e-8)\n",
        "```\n",
        "In short:\n",
        "```python\n",
        "mem += dparam * dparam\n",
        "param += -learning_rate * dparam / np.sqrt(mem + 1e-8) # adagrad update \n",
        "```\n",
        "\n",
        "### Smooth_loss\n",
        "\n",
        "Smooth_loss doesn't play any role in the training.\n",
        "It is just a low pass filtered version of the loss:\n",
        "```python\n",
        "smooth_loss = smooth_loss * 0.999 + loss * 0.001\n",
        "```\n",
        "\n",
        "It is a way to average the loss on over the last iterations to better track the progress\n",
        "\n",
        "\n",
        "### So finally\n",
        "Here the code of the main loop that does both trainning and generating text from times to times:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "HnVRQZJ8C8L9",
        "outputId": "dbe6e80a-f169-412c-c738-f7e2f183eee0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "iter 0, loss: 109.550663\n",
            "----\n",
            " qt:xskT?um65U@*!S3d$9?cl:pKm)Jk,b?ut6z%z@fLk8%x7\n",
            "lj\n",
            "tP;uรงรง\n",
            "QCesGv1Qe\"u!G5J0N4rqwvMaWรงkmXmC:bJ%OeqohJ8M;UรงcxO \"jP,@hsfLeHbpzw.w,J ,รงY8Q*DqfCHรงep ((Hrqi*oM?j8PaumA%x'ifwEG;9-*7w8?!%H\n",
            "b? Ko22h%u.;LJo6t-I \n",
            "----\n",
            "iter 1000, loss: 84.288862\n",
            "----\n",
            "  cower to mhe wifedouy hete abe birs kis, sos lf ann ho the ausid senr\" eg tot af pare movit cceec m riny he ane seila sork feiee o hy -imrer omhersive t ind wot ne we lur om jomp li cake sie , the cl \n",
            "----\n",
            "iter 2000, loss: 66.835623\n",
            "----\n",
            " vore somt ruth whous yher his aeguld wher osswe coulhisi houthen thehe wo nilcomare wive hed enet withe hes arerein. caveu that, Nofo he tuchad iod, has owther. Ifre dcverins anetht soum shent. qut. G \n",
            "----\n",
            "iter 3000, loss: 58.111046\n",
            "----\n",
            " sthad wrtwe rimiytereay whed bumago llethe paine pule liniclacrob rems byin shep, thed romk in \"reill ht on thy She ducunnindy hourlchargave waplor; cerehi!derimnkly her it ciit his parwer, beicryor o \n",
            "----\n",
            "iter 4000, loss: 53.732454\n",
            "----\n",
            "  He hot coof on that ther to thay the aver das,; oomen, - his ipy the dint bither oooth theeatwonger wathiof aine, hirn iing thouxn urren meeverered vesen mout enom to shat hivg Gregor hed feruche pey \n",
            "----\n",
            "iter 5000, loss: 55.720832\n",
            "----\n",
            " im itowr\n",
            "Ogiweppiveructmerasterlegore wrotherd. 1is abn on ather of in rof the\n",
            "Son ou con witwobfris wome * anding por. shatedathing c)ring whist 1enin. 1ermed\n",
            "s tharpdand\n",
            "be on arryorjuts recads\n",
            "1iw  \n",
            "----\n",
            "iter 6000, loss: 57.423528\n",
            "----\n",
            " . panuwirgore rmisat re t haprowlus the xeand was eve onhen noorrand ons he whev ainiberbope ofstronlmeds ins forkly aluno ab ur onin peathen hess thraghere the as afemelund ant tathe\n",
            "roaghe fro theng \n",
            "----\n",
            "iter 7000, loss: 53.527972\n",
            "----\n",
            " ictor waok.\n",
            "\n",
            "Wins adingad of and rooregrwhint ous nondut pread to rowe mfowed in cowat nougad tow hat be poulf bey at tor the ha ha dith haud on loss be this belp, to whe iarde aldingorandad on, Grego \n",
            "----\n",
            "iter 8000, loss: 50.534222\n",
            "----\n",
            " the loore wey fos thimteay hid founis tiat quha seaseron everk on he wnot eron to hovingot bet ande the finked wely be le of ligsef aered comed leching at fom lenbed wist meas ave aid tos has Equis la \n",
            "----\n",
            "iter 9000, loss: 49.007967\n",
            "----\n",
            " incitherstlecinis doomed metistronted to the to ctened andingigtanvef hare, letyed shemly hithed instas Buove, ongher sive, womenf go inted veching he homely teoutsengidile the fald, a, andiok's dougn \n",
            "----\n",
            "iter 10000, loss: 48.376684\n",
            "----\n",
            " ing drand iis thet inored bund Gregor hack clyen incy orean cere onnomed bun ho room sfovey. Grigo bllenm her in iin made oned of tortwer y yoend be the gals. Ard laved serent kten. Gret to necents th \n",
            "----\n",
            "iter 11000, loss: 54.509223\n",
            "----\n",
            " ws.\n",
            "\n",
            "OCTt5nt eralows 2ace\n",
            "whe\n",
            "hous becort\n",
            "ppune telt paef chad. leore\n",
            "s-tapd abely\n",
            "b\n",
            "fuge/ris eits cbuchred a carishiry\n",
            "hade noprmser serce thad\n",
            "\n",
            "\"W\n",
            "S s Ploprod what coslect.  Wedy.\n",
            " The W\n",
            "Ple, crath  \n",
            "----\n",
            "iter 12000, loss: 51.469354\n",
            "----\n",
            " dy of eaply can frss fadle. nom the wore the ewain doull \"Bust it wout thawd, and fasis domatew in the everly, of woos revt loagham; had staghk cowe strrelaw? He grothere sepere buin tut lould wes. wa \n",
            "----\n",
            "iter 13000, loss: 48.350148\n",
            "----\n",
            " ainay harko mibpen the keor's, smeld at toos; the chullyed yoowhrea but plyove fathtl forent haved suls abfutemespmedo sterhst the, and.z.\n",
            "\"Strogmenoot in foling waskto him theeted hull to wst as he h \n",
            "----\n",
            "iter 14000, loss: 46.481771\n",
            "----\n",
            " had wayed the for - intpimsy. \"Gregor comiof, hard fered whirgawa palasly beanett noudin the kanchiof hel at onf at f the sontepf ourawly seayingow and we hrenially wery tiok havarey'\" to hen some s c \n",
            "----\n",
            "iter 15000, loss: 45.836403\n",
            "----\n",
            " so fratay was beck, bare suritcout bery surcont, toored his stard mirhing, had mading aid mith in taplenthen, goured aint noomet ut thry to tide nougf, turistwas if the kfont and Lay; tister him, fith \n",
            "----\n",
            "iter 16000, loss: 48.527730\n",
            "----\n",
            "  and comed uning Praim: Proten calf of thiningt\n",
            "o- c speateontmenm eUnt: tertibounyent the\n",
            "y, Gregor the reyticuspouss rot \"1oled indieagipserercove petter\n",
            "persing his s)ew more. Proling tiousmencedt\n",
            " \n",
            "----\n",
            "iter 17000, loss: 50.272712\n",
            "----\n",
            "  reten, himebim. no aly he was reer thiod fron, malpoor funnen of becon\" he lowe. If clmyenn to he wave his comprest horated frrain would efrow volo, at frrale he hack abe he gordac\" Gut he houghe his \n",
            "----\n",
            "iter 18000, loss: 47.478069\n",
            "----\n",
            " n harted heretor beedir and would blly as cungroin hinsle bued he she hell thail the cith onetd a cowws kire bred this lothickle dork the, ha dout fapow. We mos it prous ay is ofatung mull. Pelynt and \n",
            "----\n",
            "iter 19000, loss: 45.416956\n",
            "----\n",
            " bee ifder thait, athrten this stapfatincon the sodyying a the rexsed haded where dids, as furnt lesary dideing as was out hut exched ing, and daver a and has to clested as ugutly wam, his dals\n",
            "yo her  \n",
            "----\n",
            "iter 20000, loss: 44.639195\n",
            "----\n",
            " and it anpthadly forchent prat; - her mo hod she bechisst chetearis aglivat could he couch swerten ther from as he just at onchle menil the das alle tho chaint. Her, eafcoughed no had dooked oplasisin \n",
            "----\n",
            "iter 21000, loss: 44.365447\n",
            "----\n",
            " r igo dicheat that \"'rnourd beed ale. Sherdly they\" Gor ly fasaraged was not hied stakpplee gerg and himed alaghel it undemoned ute eprat the cherm on that fol them efowraind, tood a the dastlie way s \n",
            "----\n",
            "iter 22000, loss: 49.692858\n",
            "----\n",
            " tifeworay to Yre fallroborent beRs. .Beghert fmaid work bowcod if ane enstrI Yoorsents aration enfor yovpadion -sain thaw padwsale conke, ofter/akmonf reit\n",
            "onmats bus bmenow\n",
            "of the coipechst F or a Sh \n",
            "----\n",
            "iter 23000, loss: 47.615616\n",
            "----\n",
            " erpllying the se wres soraght hit epusilitht carpug of from to and drok. He mare on dee hist agatiog he gremend. FAt ourd of thet had dly dide in wish tool\". Ye segly tram he deutiece. .y from theived \n",
            "----\n",
            "iter 24000, loss: 45.081926\n",
            "----\n",
            " sed begore buent dreving worcus anw wore an he was teof full myer warnigh as eving, mathing courd in creargonely he the samis sleater she was bow woring his coutene somrer whing he oulstrreay the le d \n",
            "----\n",
            "iter 25000, loss: 43.526794\n",
            "----\n",
            "  eareastly cas awabpaint with slamled behe, onl tha coulding wonquinly he was mor but yougeliny he swouble overion a liok he grayid bely pangrmys. He with him the not or hin a sto chafe andes to on th \n",
            "----\n",
            "iter 26000, loss: 43.177329\n",
            "----\n",
            " lftard int ctnoughly he rmock, Gregor's souch theren, clictole goven, wese thinknt. He his efrint, - his mother,, Gregor's somlrielling at it in the women, beer to enort watind even mont. The rpote th \n",
            "----\n",
            "iter 27000, loss: 45.552540\n",
            "----\n",
            " ugingene tike: Mrecateres itenadepentcow-r, forookedert\n",
            "ine germect Projeck to sa clyllissed reitabuster.\n",
            "\n",
            "Neatil - ocetisebert ancaw an the andyes ataiviouly oprousperm\n",
            "Prasateverg-tioung on side om  \n",
            "----\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-51b154abda12>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m   \u001b[0;31m# forward seq_length characters through the net and fetch gradient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m   \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdWxh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdWhh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdWhy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdbh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdby\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhprev\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlossFun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhprev\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m   \u001b[0msmooth_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msmooth_loss\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m0.999\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m0.001\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-10-c35a687c6a3d>\u001b[0m in \u001b[0;36mlossFun\u001b[0;34m(inputs, targets, hprev)\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0mxs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# encode in 1-of-k representation (we place a 0 vector as the t-th input)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0mxs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;31m# Inside that t-th input we use the integer in \"inputs\" list to  set the correct\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m     \u001b[0mhs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtanh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mWxh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mWhh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mbh\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# hidden state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m     \u001b[0mys\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mWhy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mby\u001b[0m \u001b[0;31m# unnormalized log probabilities for next chars\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0mps\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mys\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mys\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# probabilities for next chars\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mdot\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "n, p = 0, 0\n",
        "mWxh, mWhh, mWhy = np.zeros_like(Wxh), np.zeros_like(Whh), np.zeros_like(Why)\n",
        "mbh, mby = np.zeros_like(bh), np.zeros_like(by) # memory variables for Adagrad                                                                                                                \n",
        "smooth_loss = -np.log(1.0/vocab_size)*seq_length # loss at iteration 0                                                                                                                        \n",
        "while n<=1000*100:\n",
        "  # prepare inputs (we're sweeping from left to right in steps seq_length long)\n",
        "  # check \"How to feed the loss function to see how this part works\n",
        "  if p+seq_length+1 >= len(data) or n == 0:\n",
        "    hprev = np.zeros((hidden_size,1)) # reset RNN memory                                                                                                                                      \n",
        "    p = 0 # go from start of data                                                                                                                                                             \n",
        "  inputs = [char_to_ix[ch] for ch in data[p:p+seq_length]]\n",
        "  targets = [char_to_ix[ch] for ch in data[p+1:p+seq_length+1]]\n",
        "\n",
        "  # forward seq_length characters through the net and fetch gradient                                                                                                                          \n",
        "  loss, dWxh, dWhh, dWhy, dbh, dby, hprev = lossFun(inputs, targets, hprev)\n",
        "  smooth_loss = smooth_loss * 0.999 + loss * 0.001\n",
        "\n",
        "  # sample from the model now and then                                                                                                                                                        \n",
        "  if n % 1000 == 0:\n",
        "    print ('iter %d, loss: %f' % (n, smooth_loss)) # print progress\n",
        "    sample(hprev, inputs[0], 200)\n",
        "\n",
        "  # perform parameter update with Adagrad                                                                                                                                                     \n",
        "  for param, dparam, mem in zip([Wxh, Whh, Why, bh, by],\n",
        "                                [dWxh, dWhh, dWhy, dbh, dby],\n",
        "                                [mWxh, mWhh, mWhy, mbh, mby]):\n",
        "    mem += dparam * dparam\n",
        "    param += -learning_rate * dparam / np.sqrt(mem + 1e-8) # adagrad update                                                                                                                   \n",
        "\n",
        "  p += seq_length # move data pointer                                                                                                                                                         \n",
        "  n += 1 # iteration counter    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zd1OEomeC8L-"
      },
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 2",
      "language": "python",
      "name": "python2"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.12"
    },
    "colab": {
      "name": "SRRNN.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}