{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNhDIxp/ad+qPTPH6GsocA0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/urmilapol/urmilapolprojects/blob/master/BookSummarizer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://www.analyticsvidhya.com/blog/2023/05/create-book-summarizer-in-python-with-gpt-3-5-in-10-minutes/"
      ],
      "metadata": {
        "id": "Fxwnhh-u6JVd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Create Book Summarizer in Python with GPT-3.5 in 10 Minutes**\n",
        "\n",
        "Steps on How to Create a Book Summarizer Using Python"
      ],
      "metadata": {
        "id": "rtJH7NwuuaLN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Setting up the Environment\n",
        "To begin with, we will be using Google Colab for this purpose. Generally, the books we download are in pdf format, and for this purpose, we will install the PyPDF2 library. Besides this, we will use OpenAI’s GPT-3.5 for the task at hand, and to query it, we will use OpenAI APIs. Let’s install the required libraries:"
      ],
      "metadata": {
        "id": "LQpAuPPQusS7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c2-99OIcuB2d",
        "outputId": "7f2c25d0-16ac-4e6a-ec39-3655f1a1a57b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting openai\n",
            "  Downloading openai-0.27.8-py3-none-any.whl (73 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.6/73.6 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting PyPDF2\n",
            "  Downloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests>=2.20 in /usr/local/lib/python3.10/dist-packages (from openai) (2.27.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai) (4.65.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from openai) (3.8.4)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (2023.5.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (3.4)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (23.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (4.0.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.3.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.3.1)\n",
            "Installing collected packages: PyPDF2, openai\n",
            "Successfully installed PyPDF2-3.0.1 openai-0.27.8\n"
          ]
        }
      ],
      "source": [
        "!pip install openai PyPDF2"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Initializing Libraries\n",
        "After installing the required libraries, let’s import them and initialize the local variables. We will also need to specify the location of the pdf file and add our OpenAI API key."
      ],
      "metadata": {
        "id": "3RDQO7Tpu2dZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "import PyPDF2\n",
        "import os\n",
        "import pandas as pd\n",
        "import time\n",
        "filepath= \"/content/sample_data/Big_Data_Analytics.pdf\"\n",
        "#openai.api_key  = \"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\"\n",
        "openai.api_key  = \"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\"\n"
      ],
      "metadata": {
        "id": "4-uJO6HDu-k9"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Setup for Querying the API\n",
        "Now we will create a function for querying the GPT-3.5 Turbo model:"
      ],
      "metadata": {
        "id": "GTrSasQG2kEy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_completion(prompt, model=\"gpt-3.5-turbo\"):\n",
        "  messages = [{\"role\": \"user\", \"content\": prompt}]\n",
        "  response = openai.ChatCompletion.create(\n",
        "     model=model,\n",
        "     messages=messages,\n",
        "     temperature=0, # this is the degree of randomness of the model's output\n",
        "  )\n",
        "  return response.choices[0].message[\"content\"]"
      ],
      "metadata": {
        "id": "R79yr2h41VZ1"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Reading the PDF\n",
        "Because OpenAI has a limit on the input prompt size, we would like to send the data to be summarized in parts. There can be multiple ways to split the text. For the sake of simplicity, we will divide the whole book on the basis of pages. A better strategy will be to split it on the basis of paragraphs. However, it will increase the number of API calls increasing the overall time.\n",
        "\n",
        "We will store each page in a list and then summarize it."
      ],
      "metadata": {
        "id": "bTPwV9LX2qsC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# creating a pdf file object\n",
        "pdfFileObject = open(filepath, 'rb')\n",
        "# creating a pdf reader object\n",
        "pdfReader = PyPDF2.PdfReader(pdfFileObject)\n",
        "text=[]\n",
        "summary=' '\n",
        "#Storing the pages in a list\n",
        "for i in range(0,len(pdfReader.pages)):\n",
        "  # creating a page object\n",
        "  pageObj = pdfReader.pages[i].extract_text()\n",
        "  pageObj= pageObj.replace('\\t\\r','')\n",
        "  pageObj= pageObj.replace('\\xa0','')\n",
        "  # extracting text from page\n",
        "  text.append(pageObj)"
      ],
      "metadata": {
        "id": "LwfCElzy2zdb"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Prompting\n",
        "Now we will start prompting. This is a matter of experiment to figure out the best prompt. However, there are a few basic guidelines on how to do it efficiently. In some upcoming articles, we will discuss the art of prompting in more detail. You can use the prompt for now, which has worked well for me. You can also play around with it:"
      ],
      "metadata": {
        "id": "tVSlDqAx36nb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "result = []\n",
        "for i in range(len(text)):\n",
        "  prompt =f\"\"\"\n",
        "  Your task is to extract relevant information from a text on the page of a book. This information will be used to create a book summary.\n",
        "  Extract relevant information from the following text, which is delimited with triple backticks.\\\n",
        "  Be sure to preserve the important details.\n",
        "  Text: ```{text[i]}```\n",
        "  \"\"\"\n",
        "  try:\n",
        "    response = get_completion(prompt)\n",
        "  except:\n",
        "    response = get_completion(prompt)\n",
        "  print(response)\n",
        "  summary= summary+' ' +response +'\\n\\n'\n",
        "  result.append(response)\n",
        "  time.sleep(19)  #You can query the model only 3 times in a minute for free, so we need to put some delay\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "TJFWjLrG37uh",
        "outputId": "fa998431-e5b3-4ee0-b53d-f73a556481d8"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The book is titled \"Big Data Analytics: Disruptive Technologies for Changing the Game\" and is written by Dr. Arvind Sathi. The author is a World Wide Communication Sector architect for the Information Agenda team at IBM. The book provides a practitioner's view on Big Data Analytics and is based on workshops and interviews with business and IT leaders. It covers topics such as understanding Big Data, building a strategic plan for Big Data Analytics, and the impact of Big Data on analytics architecture. The book also respects organizations' current investments in Data Warehousing and Business Intelligence and offers evolutionary, revolutionary, and hybrid approaches to moving forward with Big Data. The price of the book is $16.95 US/$18.95 CAN.\n",
            "The relevant information extracted from the text is as follows:\n",
            "\n",
            "- Publisher: MC Press Online, LLC\n",
            "- Location: Boise, ID 83703\n",
            "- Author: Dr. Arvind Sathi\n",
            "The relevant information extracted from the text is as follows:\n",
            "\n",
            "Title: Big Data Analytics: Disruptive Technologies for Changing the Game\n",
            "Author: Dr. Arvind Sathi\n",
            "Edition: First Edition\n",
            "Printing: First Printing — October 2012\n",
            "Publisher: IBM Corporation\n",
            "Trademarks: IBM, Big Insights, Cognos, DB2, Entity Analytics, InfoSphere, Netezza, NPS, Optim, pureScale, SlamTracker, Smarter Cities, SPSS, Streams, Unica, Vivisimo, z/OS, TEALEAF, WORKLIGHT\n",
            "Other Trademarks: Adobe, Linux, Microsoft, Windows, Java, Oracle\n",
            "Copyright: © 2012 IBM Corporation. All rights reserved.\n",
            "Printed in: Canada\n",
            "Publisher's Address: MC Press Online, LLC, 3695 W Quail Heights Court, Boise, ID 83703-3861 USA\n",
            "Customer Service: Toll Free: (877) 226-5394; cust.srv@mcpressonline.com\n",
            "Permissions and Special/Bulk Orders: mcbooks@mcpressonline.com\n",
            "ISBN: 978-1-58347-380-1\n",
            "The book is dedicated to Professor Herbert Simon, who sparked the author's curiosity in qualitative reasoning. The author also acknowledges Neena, Kinji, Kevin, and Conal for their time, encouragement, and support in writing the book.\n",
            "The relevant information extracted from the text is as follows:\n",
            "\n",
            "- The author's name is Dr. Arvind Sathi.\n",
            "- Dr. Sathi is the World Wide Communication Sector architect for the Information Agenda team at IBM.\n",
            "- He received his Ph.D. in Business Administration from Carnegie Mellon University.\n",
            "- Dr. Sathi worked under Nobel Prize winner Dr. Herbert A. Simon.\n",
            "- He has more than 20 years of leadership experience in Information Management architecture and delivery.\n",
            "- Dr. Sathi's primary focus has been on creating visions and roadmaps for Advanced Analytics at leading IBM clients in telecommunications, media and entertainment, and energy and utilities organizations worldwide.\n",
            "- He has conducted workshops on Big Data assessment and roadmap development.\n",
            "- Before joining IBM, Dr. Sathi developed knowledge-based solutions for CRM at Carnegie Group.\n",
            "- At BearingPoint, he led the development of Enterprise Integration, MDM, and Operations Support Systems/Business Support Systems (OSS/BSS) solutions for the communications market.\n",
            "- Dr. Sathi also developed horizontal solutions for communications, financial services, and public services.\n",
            "- At IBM, he has led several Information Management programs in MDM, data security, business intelligence, and related areas.\n",
            "- Dr. Sathi has provided architecture oversight to IBM's strategic accounts.\n",
            "- He has delivered workshops and presentations at industry conferences on technical subjects including MDM and data architecture.\n",
            "- Dr. Sathi holds two patents in data masking.\n",
            "- His first book, \"Customer Experience Analytics,\" was released by MC Press in October 2011.\n",
            "- He has also been a contributing author in a number of Data Governance books written by Sunil Soares.\n",
            "The relevant information extracted from the text is as follows:\n",
            "\n",
            "- The author acknowledges the hard work of the Information Agenda community in creating a world-class reference material.\n",
            "- The author heavily referenced the Business Maturity Model, the Solution Architecture framework, and a number of case studies.\n",
            "- Bob Keseley, Wayne Jensen, and Mick Fullwood are acknowledged for conceiving the ideas and organizing the reference material.\n",
            "- Tim Davis is acknowledged for his encouragement and for providing financial services examples.\n",
            "- Jeff Jonas provided inspiration and much of the backbone for the book.\n",
            "- The technical ideas were created with help from Beth Brownhill, Paul Christensen, Elizabeth Dial, Ram Dorairaj, Tommy Eunice, Rich Harken, Eberhard Hechler, Bob Johnston, Noman Mohammed, Peter Harrison, Daryl BC Peh, Steve Rigo, and Barry Rosen.\n",
            "- The Dallas Global Solutions Center team (Christian Loza, Tom Slade, Mathews Thomas, and Janki Vora) provided valuable experimentations on the ideas.\n",
            "- Mehul Shah, Emeline Tjan, Livio Ventura, Wolfgang Bosch, Steve Trigg, Don Bahash, and Jessica White provided valuable business value analysis components in the book.\n",
            "- The Communication Sector Industry Consulting team (Ken Kralick, Dirk Michelsen, Tushar Mehta, Richard Lanahan, Rick Flamand, Linda Moss, and David Buck) provided opportunities, customers, and contributions to the Big Data Analytics solutions.\n",
            "- The IBM Business Analytics and Optimization consulting team (Adam Gersting, Joseph Baird, Anu Jain, Bruce Weiss, Aparna Betigeri, and John Held) provided the ideas behind the business scenarios and use cases.\n",
            "- Mark Holste collaborated and brainstormed on the solutions.\n",
            "- The IBM Software Group product teams provided case studies and product examples.\n",
            "- Roger Rea, Dan Debrunner, and Vibhor Kumar helped with the InfoSphere Streams product.\n",
            "- Arun Manoharan and Patrick Welsh supported in getting Vivisimo information.\n",
            "- Andrew Colby helped with the Netezza Analytics Engine.\n",
            "- Shankar Venkataraman, Girish Venkatachaliah, and Karthik Hariharan contributed to Big Insights.\n",
            "- Claudio Zancani contributed to Optim Privacy.\n",
            "- Mike Zucker contributed to SPSS.\n",
            "- The author worked closely with practitioners, including Anthony Behan, Ash Kanagat, and Audrey Laird, while studying Big Data business opportunities.\n",
            "The text mentions several individuals who have contributed to the book. These individuals include:\n",
            "\n",
            "- Bob Weiss\n",
            "- Christine Twiford\n",
            "- Carmen Allen\n",
            "- Dave Dunmire\n",
            "- Doug Humfries\n",
            "- Duane Gabor\n",
            "- Gautam Shah\n",
            "- Girish Varma\n",
            "- Harpinder Singh Madan\n",
            "- Harsch Bhatnagar\n",
            "- Jay Praturi\n",
            "- Jessica Shah\n",
            "- Jim Hicks\n",
            "- Joshua Koran\n",
            "- Judith List\n",
            "- Kedrick Brown\n",
            "- Ken Babb\n",
            "- Lindsey Pardun\n",
            "- Mahesh Dalvi\n",
            "- Maureen Little\n",
            "- Neil Isford\n",
            "- Norbert Herman\n",
            "- Oliver Birch\n",
            "- Perry McDonald\n",
            "- Philip Smolin\n",
            "- Piyush Sarwal\n",
            "- Ravi Kothari\n",
            "- Randy George\n",
            "- Raquel Katigbak\n",
            "- Richa Pandey\n",
            "- Rob Smith\n",
            "- Robert Segat\n",
            "- Sam King\n",
            "- Sankar Virdhagriswaran\n",
            "- Sara Philpott\n",
            "- Steve Cohen\n",
            "- Steve Teitzel\n",
            "- Sumit Chowdhury\n",
            "- Sumit Singh\n",
            "- Teresa Jacobs\n",
            "- Umadevi Reddy\n",
            "- Vasco Queiros\n",
            "- Vikas Pathuri\n",
            "- V on McConnell\n",
            "- Yoel Arditi\n",
            "\n",
            "The text also mentions the following individuals who have provided specific contributions:\n",
            "\n",
            "- Cheryl Daugherty (reviewed the book)\n",
            "- Sunil Soares (inspired the author to write the book)\n",
            "- Gaurav Deshpande (helped organize and fund the book, co-authored the cartoon strip)\n",
            "- Susan Visser (provided help organizing the publication process)\n",
            "- Katie Tipton (provided publication and editorial guidance)\n",
            "\n",
            "Lastly, the author expresses gratitude towards their family members for their inspiration, support, and editorial help. These family members include:\n",
            "\n",
            "- Wife: Neena\n",
            "- Daughter: Kinji\n",
            "- Son-in-law: Kevin\n",
            "- Son: Conal\n",
            "The text is a foreword written by Bob Keseley for the book \"Big Data Analytics: Disruptive Technologies for Changing the Game\" by Arvind. In the foreword, Keseley mentions the increasing interest in Big Data Analytics globally and how top performers have declared themselves \"Analytics driven\" organizations. He highlights that successful organizations are focused on Big Data use cases and techniques that drive the greatest business value, rather than just the tools or products. Keseley mentions that the book explores best practices across sales, marketing, customer service, and risk management, and links them to the solutions and architectures that make it all possible. He hopes that the book will facilitate the right dialogue between business and IT leaders.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-59bec705c480>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m   \u001b[0msummary\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0msummary\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m' '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0mresponse\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m'\\n\\n'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m   \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m   \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m19\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m#You can query the model only 3 times in a minute for free, so we need to put some delay\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Saving the Summary\n",
        "Finally, we will save the summary we have obtained in a text file:"
      ],
      "metadata": {
        "id": "RMsoJhw8_5iz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open('summary.txt', 'w') as out:\n",
        "  out.write(summary)"
      ],
      "metadata": {
        "id": "nplzZmVY_4yt"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}