{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMHGn9Ga272kg0kp3EM771M",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/urmilapol/urmilapolprojects/blob/master/DE.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Explanation of the Python Data Pipeline Code\n",
        "The provided Python script (pipeline.py) serves as a practical demonstration of a simple Data Pipeline and the Data Flow that occurs within it. It uses the pandas library to process simulated sales order data, illustrating the core concepts of data engineering: Extraction, Transformation, and Loading (ETL/ELT).\n",
        "\n",
        "1. The Data Pipeline: Orchestration (The \"How\" and \"When\")\n",
        "The Data Pipeline is represented by the run_pipeline() function, which acts as the orchestrator. Its primary role is to define the sequence of operations and manage the flow of control between the different stages.\n",
        "\n",
        "Function\tPipeline Stage\tRole in Orchestration\n",
        "run_pipeline()\tOrchestrator\tCalls the stages in the correct order: extract_data → transform_data → load_data. It ensures the entire process executes reliably from start to finish.\n",
        "extract_data()\tExtract/Ingestion\tThe starting point. Responsible for connecting to the data source (simulated by reading a CSV file) and bringing the raw data into memory (pandas.DataFrame).\n",
        "load_data()\tLoad/Storage\tThe endpoint. Responsible for taking the processed data and writing it to the final storage locations (simulated by writing to two separate CSV files).\n",
        "This structure ensures that the pipeline is modular, allowing each stage to be developed, tested, and monitored independently.\n",
        "\n",
        "2. The Data Flow: Transformation Logic (The \"What\" and \"Where\")\n",
        "The Data Flow is primarily contained within the transform_data() function. This function defines the specific sequence of steps and logic applied to the data to convert it from its raw state into a business-ready format.\n",
        "\n",
        "The code demonstrates a multi-step data flow:\n",
        "\n",
        "Step 2.1: Cleansing and Validation\n",
        "The first part of the transformation ensures data quality:\n",
        "df['Amount'] = pd.to_numeric(df['Amount'], errors='coerce')\n",
        "df.dropna(subset=['Amount'], inplace=True)\n",
        "This logic cleans the Amount column by converting it to a numeric type. If any value cannot be converted (e.g., \"N/A\" or \"Error\"), it is set to NaN and then dropped, ensuring that only valid, numeric data proceeds to the next steps.\n",
        "\n",
        "Step 2.2: Filtering (Business Rule Application)\n",
        "A specific business rule is applied to the data flow:\n",
        "cleansed_df = df[df['Amount'] >= MIN_ORDER_AMOUNT].copy()\n",
        "This step filters the dataset, keeping only orders where the Amount is greater than or equal to $100 (defined by MIN_ORDER_AMOUNT). This simulates a common scenario where a pipeline filters out irrelevant or low-value transactions before further processing. The output cleansed_df represents a Curated Layer of data.\n",
        "\n",
        "Step 2.3: Aggregation (Data Enrichment)\n",
        "The final step of the data flow aggregates the filtered data:\n",
        "aggregated_df = cleansed_df.groupby('CustomerID').agg(...)\n",
        "This logic groups the data by CustomerID and calculates key metrics: the total number of orders, the total sales amount, and the average order value. This transformation creates a summary dataset (aggregated_df) that is optimized for reporting and analytics, representing a Presentation Layer of data.\n",
        "\n",
        "3. Simulating Storage Layers\n",
        "The load_data() function demonstrates how a single pipeline can feed multiple storage layers, a key concept in modern data architectures (like the Data Lakehouse):\n",
        "\n",
        "1\tcleansed_orders.csv: Stores the filtered, row-level data. This is analogous to a Silver Layer—clean, consistent, and ready for detailed analysis.\n",
        "2\tcustomer_sales_summary.csv: Stores the aggregated, summarized data. This is analogous to a Gold Layer—highly refined, business-ready, and optimized for dashboards and reports.\n",
        "\n",
        "In summary, the Python script clearly separates the Pipeline (the sequence of function calls) from the Data Flow (the logic inside transform_data), providing a concrete, executable example of the concepts discussed in your syllabus.\n"
      ],
      "metadata": {
        "id": "GboMWL32Nj_Q"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dGaPPXdT9zHM",
        "outputId": "a8deddf6-f439-4cac-aebd-24f6b7c5c0e8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[11:35:13] Starting Stage 1: Extracting data from /content/sample_data/source_data.csv...\n",
            "Successfully extracted 10 records.\n",
            "[11:35:13] Starting Stage 2: Transforming data...\n",
            "   - Cleansing complete. Remaining records: 10\n",
            "   - Filtered orders below $100. Records for analysis: 5\n",
            "   - Aggregation complete. Generated summary for 4 customers.\n",
            "[11:35:13] Starting Stage 3: Loading data...\n",
            "   - Cleansed orders loaded to /content/sample_data/cleansed_orders.csv\n",
            "   - Customer sales summary loaded to /content/sample_data/customer_sales_summary.csv\n",
            "[11:35:13] Data Pipeline execution complete.\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "from datetime import datetime\n",
        "\n",
        "# --- Configuration ---\n",
        "SOURCE_FILE = \"/content/sample_data/source_data.csv\"\n",
        "CLEANSED_FILE = \"/content/sample_data/cleansed_orders.csv\"\n",
        "AGGREGATED_FILE = \"/content/sample_data/customer_sales_summary.csv\"\n",
        "MIN_ORDER_AMOUNT = 100\n",
        "\n",
        "def extract_data(file_path: str) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Stage 1: Extraction (Simulated Ingestion/Load)\n",
        "    Reads the raw data from the source CSV file.\n",
        "    \"\"\"\n",
        "    print(f\"[{datetime.now().strftime('%H:%M:%S')}] Starting Stage 1: Extracting data from {file_path}...\")\n",
        "    try:\n",
        "        # Use pandas to read the CSV file\n",
        "        df = pd.read_csv(file_path)\n",
        "        print(f\"Successfully extracted {len(df)} records.\")\n",
        "        return df\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: Source file not found at {file_path}\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "def transform_data(df: pd.DataFrame) -> tuple[pd.DataFrame, pd.DataFrame]:\n",
        "    \"\"\"\n",
        "    Stage 2: Transformation (Data Flow Logic)\n",
        "    Performs cleansing, filtering, and aggregation.\n",
        "    \"\"\"\n",
        "    print(f\"[{datetime.now().strftime('%H:%M:%S')}] Starting Stage 2: Transforming data...\")\n",
        "\n",
        "    # 2.1 Cleansing: Ensure 'Amount' is numeric and handle potential errors\n",
        "    df['Amount'] = pd.to_numeric(df['Amount'], errors='coerce')\n",
        "    df.dropna(subset=['Amount'], inplace=True)\n",
        "    print(f\"   - Cleansing complete. Remaining records: {len(df)}\")\n",
        "\n",
        "    # 2.2 Filtering: Filter out small orders (Data Flow Logic 1)\n",
        "    cleansed_df = df[df['Amount'] >= MIN_ORDER_AMOUNT].copy()\n",
        "    print(f\"   - Filtered orders below ${MIN_ORDER_AMOUNT}. Records for analysis: {len(cleansed_df)}\")\n",
        "\n",
        "    # 2.3 Aggregation: Calculate total sales per customer (Data Flow Logic 2)\n",
        "    aggregated_df = cleansed_df.groupby('CustomerID').agg(\n",
        "        Total_Orders=('OrderID', 'count'),\n",
        "        Total_Sales=('Amount', 'sum'),\n",
        "        Average_Order_Value=('Amount', 'mean')\n",
        "    ).reset_index()\n",
        "\n",
        "    print(f\"   - Aggregation complete. Generated summary for {len(aggregated_df)} customers.\")\n",
        "\n",
        "    return cleansed_df, aggregated_df\n",
        "\n",
        "def load_data(cleansed_df: pd.DataFrame, aggregated_df: pd.DataFrame):\n",
        "    \"\"\"\n",
        "    Stage 3: Load (Simulated Storage)\n",
        "    Writes the transformed data to destination CSV files.\n",
        "    \"\"\"\n",
        "    print(f\"[{datetime.now().strftime('%H:%M:%S')}] Starting Stage 3: Loading data...\")\n",
        "\n",
        "    # Load 1: Cleansed data (Simulating a Silver/Curated layer)\n",
        "    cleansed_df.to_csv(CLEANSED_FILE, index=False)\n",
        "    print(f\"   - Cleansed orders loaded to {CLEANSED_FILE}\")\n",
        "\n",
        "    # Load 2: Aggregated summary (Simulating a Gold/Presentation layer)\n",
        "    aggregated_df.to_csv(AGGREGATED_FILE, index=False)\n",
        "    print(f\"   - Customer sales summary loaded to {AGGREGATED_FILE}\")\n",
        "\n",
        "    print(f\"[{datetime.now().strftime('%H:%M:%S')}] Data Pipeline execution complete.\")\n",
        "\n",
        "def run_pipeline():\n",
        "    \"\"\"\n",
        "    Orchestrates the entire data pipeline process.\n",
        "    \"\"\"\n",
        "    # 1. Extract/Load\n",
        "    raw_data = extract_data(SOURCE_FILE)\n",
        "    if raw_data.empty:\n",
        "        print(\"Pipeline aborted due to extraction error.\")\n",
        "        return\n",
        "\n",
        "    # 2. Transform (The Data Flow)\n",
        "    cleansed_data, aggregated_summary = transform_data(raw_data)\n",
        "\n",
        "    # 3. Load\n",
        "    load_data(cleansed_data, aggregated_summary)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    run_pipeline()\n"
      ]
    }
  ]
}