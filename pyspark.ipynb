{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOvjFW6++LdNeLzCsgPdPb1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/urmilapol/urmilapolprojects/blob/master/pyspark.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://www.chaosgenius.io/blog/apache-spark-architecture/\n"
      ],
      "metadata": {
        "id": "Sk0n1hnvUxGg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xvBykNEPSOgp",
        "outputId": "37965d54-cb41-4d67-bd15-d3da7a26e87f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+----+--------+\n",
            "|CourseName|fee |discount|\n",
            "+----------+----+--------+\n",
            "|Java      |4000|5       |\n",
            "|Python    |4600|10      |\n",
            "|Scala     |4100|15      |\n",
            "+----------+----+--------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import upper\n",
        "\n",
        "spark = SparkSession.builder.appName(\"DataTransformation\").getOrCreate()\n",
        "\n",
        "# Sample data: courses with fees and discounts\n",
        "data = [(\"Java\", 4000, 5), (\"Python\", 4600, 10), (\"Scala\", 4100, 15)]\n",
        "columns = [\"CourseName\", \"fee\", \"discount\"]\n",
        "df = spark.createDataFrame(data, columns)\n",
        "df.show(truncate=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def to_upper(df):\n",
        "    return df.withColumn(\"CourseName\", upper(df.CourseName))\n",
        "\n",
        "def reduce_price(df, amount):\n",
        "    return df.withColumn(\"new_fee\", df.fee - amount)\n",
        "\n",
        "def apply_discount(df):\n",
        "    return df.withColumn(\"discounted_fee\", df.new_fee * (1 - df.discount / 100))\n",
        "\n",
        "# Apply chain\n",
        "result = df.transform(to_upper).transform(reduce_price, 1000).transform(apply_discount)\n",
        "result.select(\"CourseName\", \"discounted_fee\").show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dtQV_VK2Sayi",
        "outputId": "b64c3a4e-6d09-44fa-bc6f-7ce49c46090b"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+--------------+\n",
            "|CourseName|discounted_fee|\n",
            "+----------+--------------+\n",
            "|      JAVA|        2850.0|\n",
            "|    PYTHON|        3240.0|\n",
            "|     SCALA|        2635.0|\n",
            "+----------+--------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Apache Spark excels in real-world data processing through transformations like joins, aggregations, and window functions on large datasets. Here's another hands-on PySpark example focused on sales data aggregation—a common ETL scenario for retail analytics that builds on the prior fee transformation demo**\n",
        "Sales Aggregation Example\n",
        "# This processes transactional sales data to compute daily revenue by product category, filtering invalid records and applying windowed ranking for top performers.\n",
        "This PySpark code sets up an ETL (Extract, Transform, Load) pipeline to process sales data, aggregate it by daily revenue per category, and then rank the categories within each day.\n",
        "\n"
      ],
      "metadata": {
        "id": "bWveGycCXS3q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "# It imports necessary classes and functions from pyspark.sql  which is a library for working with structured data in Spark. It includes DataFrame for various data operations, and Window for defining window functions.\n",
        "from pyspark.sql.functions import col, to_date, sum as spark_sum, rank, desc\n",
        "from pyspark.sql.window import Window\n",
        "#creates or retrieves a SparkSession, which is the entry point to Spark functionality\n",
        "spark = SparkSession.builder.appName(\"SalesAggregation\").getOrCreate()\n",
        "\n",
        "# Sample sales data (scale to CSV from Kaggle e-commerce datasets)\n",
        "#sales_data is a Python list of tuples representing raw sales records, and columns defines the schema for this data. df = spark.createDataFrame(sales_data, columns) then converts this Python data into a Spark DataFrame.\n",
        "sales_data = [\n",
        "    (\"2025-01-01\", \"Electronics\", 100, 2),\n",
        "    (\"2025-01-01\", \"Clothing\", 50, 5),\n",
        "    (\"2025-01-02\", \"Electronics\", 100, 1),\n",
        "    (\"2025-01-02\", \"Clothing\", 50, 3),\n",
        "    (\"2025-01-01\", \"Books\", 20, 10),  # Low price, high volume\n",
        "    (\"2025-01-03\", \"Books\", 20, 0)    # Invalid (zero qty)\n",
        "]\n",
        "\n",
        "columns = [\"sale_date\", \"category\", \"price\", \"quantity\"]\n",
        "df = spark.createDataFrame(sales_data, columns)\n",
        "\n",
        "# ETL Pipeline: Clean → Transform → Aggregate\n",
        "#converts the sale_date column from a string to a proper date type.\n",
        "df_clean = df.filter(col(\"quantity\") > 0).withColumn(\"sale_date\", to_date(col(\"sale_date\")))\n",
        "#aggregates the cleaned data: groups by sale_date and category, and calculates the total revenue for each combination. calculates the total revenue for each group by multiplying price and quantity and summing them up.\n",
        "df_agg = df_clean.groupBy(\"sale_date\", \"category\").agg(spark_sum(col(\"price\") * col(\"quantity\")).alias(\"revenue\"))\n",
        "\n",
        "# Window function for ranking top categories per day   This section calculates the rank of each category's revenue within each day\n",
        "#defines a window specification. It partitions the data by sale_date (meaning ranks are calculated independently for each day) and orders the results within each partition by revenue in descending order.\n",
        "#applies this window function to df_agg, creating a new column named rank that assigns a rank to each category based on its revenue within its respective day.\n",
        "window_spec = Window.partitionBy(\"sale_date\").orderBy(desc(\"revenue\"))\n",
        "df_ranked = df_agg.withColumn(\"rank\", rank().over(window_spec))\n",
        "#displays the final df_ranked DataFrame, ordered by sale_date and then by revenue in descending order, showing the top-performing categories for each day.\n",
        "\n",
        "df_ranked.orderBy(\"sale_date\", desc(\"revenue\")).show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CnZVdqdpXXWU",
        "outputId": "3e3fc96a-2b51-485b-f1ed-2b5e7dc6c674"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+-----------+-------+----+\n",
            "|sale_date |category   |revenue|rank|\n",
            "+----------+-----------+-------+----+\n",
            "|2025-01-01|Clothing   |250    |1   |\n",
            "|2025-01-01|Electronics|200    |2   |\n",
            "|2025-01-01|Books      |200    |2   |\n",
            "|2025-01-02|Clothing   |150    |1   |\n",
            "|2025-01-02|Electronics|100    |2   |\n",
            "+----------+-----------+-------+----+\n",
            "\n"
          ]
        }
      ]
    }
  ]
}