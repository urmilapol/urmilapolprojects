{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPeJG5Mu4lK2zX6GxXPh1zo",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/urmilapol/urmilapolprojects/blob/master/pyspark.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ===== COMPLETE STOCK P/E RATIO ETL PIPELINE =====\n",
        "# Video: https://www.youtube.com/watch?v=KAuIvccwbPY\n",
        "\n",
        "# 1. Initialize SparkSession (Driver Program Entry Point)\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, when, avg, count\n",
        "\n",
        "# Create SparkSession - connects to cluster (local/4 nodes/serverless)\n",
        "# Use cluster: \"yarn\", \"k8s://...\", or omit for default\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"StockPEratioDemo\") \\\n",
        "    .master(\"local[*]\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "print(\"âœ… SparkSession created. Cluster ready for distributed processing.\")\n",
        "\n",
        "# 2. EXTRACT: Load CSV (Spark auto-partitions across workers)\n",
        "df_raw = spark.read \\\n",
        "    .option(\"header\", \"true\") \\\n",
        "    .option(\"inferSchema\", \"true\") \\\n",
        "    .csv(\"/content/sample_data/stocks.csv\")  # Scales to HDFS/S3 paths\n",
        "\n",
        "print(\"ðŸ“Š Raw data schema:\")\n",
        "df_raw.printSchema()\n",
        "df_raw.show()\n",
        "\n",
        "# 3. TRANSFORM: Business logic (lazy evaluation - no execution yet)\n",
        "df_pe = df_raw.withColumn(\n",
        "    \"pe_ratio\",\n",
        "    col(\"Price\") / col(\"EPS\")\n",
        ").withColumn(\n",
        "    \"pe_category\",\n",
        "    when(col(\"pe_ratio\") < 15, \"Undervalued\")\n",
        "    .when(col(\"pe_ratio\") > 30, \"Overvalued\")\n",
        "    .otherwise(\"Fair\")\n",
        ")\n",
        "\n",
        "print(\"ðŸ”„ Transformation DAG built (lazy)\")\n",
        "\n",
        "# 4. LOAD/ACTION: Triggers execution across cluster\n",
        "print(\"ðŸš€ EXECUTING across cluster...\")\n",
        "df_result = df_pe.orderBy(col(\"pe_ratio\"))\n",
        "df_result.select(\"Symbol\", \"Price\", \"EPS\", \"pe_ratio\", \"pe_category\").show(truncate=False)\n",
        "\n",
        "# 5. Advanced: Aggregate analytics (real ETL use case)\n",
        "df_summary = df_pe.groupBy(\"pe_category\").agg(\n",
        "    avg(\"pe_ratio\").alias(\"avg_pe\"),\n",
        "    count(\"Symbol\").alias(\"stock_count\")\n",
        ").orderBy(\"pe_category\")\n",
        "print(\"ðŸ“ˆ Portfolio Summary:\")\n",
        "df_summary.show()\n",
        "\n",
        "# 6. Write results (complete ETL)\n",
        "df_result.coalesce(1).write \\\n",
        "    .mode(\"overwrite\") \\\n",
        "    .option(\"header\", \"true\") \\\n",
        "    .csv(\"stocks_pe_analysis\")\n",
        "\n",
        "print(\"ðŸ’¾ Results written to stocks_pe_analysis/\")\n",
        "\n",
        "# 7. Cleanup\n",
        "spark.stop()\n",
        "print(\"âœ… Spark cluster shutdown.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5dfc1b60-40e8-40bb-c757-1441aa3d9660",
        "id": "B5RX3Nnzvyeq"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… SparkSession created. Cluster ready for distributed processing.\n",
            "ðŸ“Š Raw data schema:\n",
            "root\n",
            " |-- Symbol: string (nullable = true)\n",
            " |-- Price: double (nullable = true)\n",
            " |-- EPS: double (nullable = true)\n",
            "\n",
            "+------+-------+-----+\n",
            "|Symbol|  Price|  EPS|\n",
            "+------+-------+-----+\n",
            "|  AAPL|  175.5| 6.16|\n",
            "|  GOOG|2850.25|145.6|\n",
            "|  MSFT|  425.8| 11.8|\n",
            "|  TSLA|  245.3| 3.65|\n",
            "|  AMZN|  186.8| 4.12|\n",
            "|  META|  567.9|20.35|\n",
            "|  NVDA| 890.45|12.05|\n",
            "+------+-------+-----+\n",
            "\n",
            "ðŸ”„ Transformation DAG built (lazy)\n",
            "ðŸš€ EXECUTING across cluster...\n",
            "+------+-------+-----+------------------+-----------+\n",
            "|Symbol|Price  |EPS  |pe_ratio          |pe_category|\n",
            "+------+-------+-----+------------------+-----------+\n",
            "|GOOG  |2850.25|145.6|19.575892857142858|Fair       |\n",
            "|META  |567.9  |20.35|27.906633906633903|Fair       |\n",
            "|AAPL  |175.5  |6.16 |28.490259740259738|Fair       |\n",
            "|MSFT  |425.8  |11.8 |36.08474576271186 |Overvalued |\n",
            "|AMZN  |186.8  |4.12 |45.33980582524272 |Overvalued |\n",
            "|TSLA  |245.3  |3.65 |67.2054794520548  |Overvalued |\n",
            "|NVDA  |890.45 |12.05|73.89626556016597 |Overvalued |\n",
            "+------+-------+-----+------------------+-----------+\n",
            "\n",
            "ðŸ“ˆ Portfolio Summary:\n",
            "+-----------+------------------+-----------+\n",
            "|pe_category|            avg_pe|stock_count|\n",
            "+-----------+------------------+-----------+\n",
            "|       Fair|25.324262168012165|          3|\n",
            "| Overvalued| 55.63157415004384|          4|\n",
            "+-----------+------------------+-----------+\n",
            "\n",
            "ðŸ’¾ Results written to stocks_pe_analysis/\n",
            "âœ… Spark cluster shutdown.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://www.chaosgenius.io/blog/apache-spark-architecture/\n",
        "\n",
        "\n",
        "Apache Spark follows a master-slave (driver-worker) architecture for distributed data processing, ideal for data engineering pipelines and interviews.\n",
        "High-Level Architecture\n",
        "Spark operates with a Driver Program (master), Cluster Manager, and Executors (workers) on cluster nodes. Tasks represent the smallest unit: a Spark job divides into stages, stages into tasks executed in parallel.[2][1]\n",
        "Cluster Manager (Standalone, YARN, Mesos, Kubernetes) allocates CPU/memory resources dynamically, upscaling for faster completion.[1]\n",
        "Master-slave: Driver coordinates; executors process; fault-tolerant via recomputation.[1]\n",
        "Driver Responsibilities\n",
        "Driver runs the main Spark application (your PySpark code), converting user code into a Directed Acyclic Graph (DAG)â€”a one-way graph of operations without loops. Requests resources from Cluster Manager, schedules tasks on executors, tracks progress, and collects results.[1]\n",
        "Key interview point: Driver handles logical-to-physical plan translation via DAG Scheduler.[1]\n",
        "Cluster Manager Role\n",
        "Manages resource allocation (e.g., CPUs, memory) to executors, monitors availability, and assigns free resources. YARN (Yet Another Resource Negotiator) is common in Hadoop ecosystems.[2][1]\n",
        "Executors and Tasks\n",
        "Executors (worker processes on nodes) execute tasks, store data in memory/disk, with dedicated CPU cores and task slots. Each holds executor memory for caching.[1]\n",
        "Task Scheduler assigns tasks to executors post-DAG breakdown.[1]\n",
        "Job Execution Flow\n",
        "â€¢\tUser submits code â†’ Driver creates logical plan.\n",
        "â€¢\tDAG Scheduler: Logical â†’ physical plan, breaks job into stages (shuffle boundaries).\n",
        "â€¢\tTask Scheduler: Stages â†’ tasks assigned to executors.\n",
        "â€¢\tCluster Manager allocates resources.\n",
        "â€¢\tExecutors run tasks in parallel â†’ Driver aggregates results for storage/output.[1]\n",
        "\n"
      ],
      "metadata": {
        "id": "Sk0n1hnvUxGg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xvBykNEPSOgp",
        "outputId": "37965d54-cb41-4d67-bd15-d3da7a26e87f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+----+--------+\n",
            "|CourseName|fee |discount|\n",
            "+----------+----+--------+\n",
            "|Java      |4000|5       |\n",
            "|Python    |4600|10      |\n",
            "|Scala     |4100|15      |\n",
            "+----------+----+--------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import upper\n",
        "\n",
        "spark = SparkSession.builder.appName(\"DataTransformation\").getOrCreate()\n",
        "\n",
        "# Sample data: courses with fees and discounts\n",
        "data = [(\"Java\", 4000, 5), (\"Python\", 4600, 10), (\"Scala\", 4100, 15)]\n",
        "columns = [\"CourseName\", \"fee\", \"discount\"]\n",
        "df = spark.createDataFrame(data, columns)\n",
        "df.show(truncate=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def to_upper(df):\n",
        "    return df.withColumn(\"CourseName\", upper(df.CourseName))\n",
        "\n",
        "def reduce_price(df, amount):\n",
        "    return df.withColumn(\"new_fee\", df.fee - amount)\n",
        "\n",
        "def apply_discount(df):\n",
        "    return df.withColumn(\"discounted_fee\", df.new_fee * (1 - df.discount / 100))\n",
        "\n",
        "# Apply chain\n",
        "result = df.transform(to_upper).transform(reduce_price, 1000).transform(apply_discount)\n",
        "result.select(\"CourseName\", \"discounted_fee\").show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dtQV_VK2Sayi",
        "outputId": "b64c3a4e-6d09-44fa-bc6f-7ce49c46090b"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+--------------+\n",
            "|CourseName|discounted_fee|\n",
            "+----------+--------------+\n",
            "|      JAVA|        2850.0|\n",
            "|    PYTHON|        3240.0|\n",
            "|     SCALA|        2635.0|\n",
            "+----------+--------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Apache Spark excels in real-world data processing through transformations like joins, aggregations, and window functions on large datasets. Here's another hands-on PySpark example focused on sales data aggregationâ€”a common ETL scenario for retail analytics that builds on the prior fee transformation demo**\n",
        "Sales Aggregation Example\n",
        "# This processes transactional sales data to compute daily revenue by product category, filtering invalid records and applying windowed ranking for top performers.\n",
        "This PySpark code sets up an ETL (Extract, Transform, Load) pipeline to process sales data, aggregate it by daily revenue per category, and then rank the categories within each day.\n",
        "\n"
      ],
      "metadata": {
        "id": "bWveGycCXS3q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "# It imports necessary classes and functions from pyspark.sql  which is a library for working with structured data in Spark. It includes DataFrame for various data operations, and Window for defining window functions.\n",
        "from pyspark.sql.functions import col, to_date, sum as spark_sum, rank, desc\n",
        "from pyspark.sql.window import Window\n",
        "#creates or retrieves a SparkSession, which is the entry point to Spark functionality\n",
        "spark = SparkSession.builder.appName(\"SalesAggregation\").getOrCreate()\n",
        "\n",
        "# Sample sales data (scale to CSV from Kaggle e-commerce datasets)\n",
        "#sales_data is a Python list of tuples representing raw sales records, and columns defines the schema for this data. df = spark.createDataFrame(sales_data, columns) then converts this Python data into a Spark DataFrame.\n",
        "sales_data = [\n",
        "    (\"2025-01-01\", \"Electronics\", 100, 2),\n",
        "    (\"2025-01-01\", \"Clothing\", 50, 5),\n",
        "    (\"2025-01-02\", \"Electronics\", 100, 1),\n",
        "    (\"2025-01-02\", \"Clothing\", 50, 3),\n",
        "    (\"2025-01-01\", \"Books\", 20, 10),  # Low price, high volume\n",
        "    (\"2025-01-03\", \"Books\", 20, 0)    # Invalid (zero qty)\n",
        "]\n",
        "\n",
        "columns = [\"sale_date\", \"category\", \"price\", \"quantity\"]\n",
        "df = spark.createDataFrame(sales_data, columns)\n",
        "\n",
        "# ETL Pipeline: Clean â†’ Transform â†’ Aggregate\n",
        "#converts the sale_date column from a string to a proper date type.\n",
        "df_clean = df.filter(col(\"quantity\") > 0).withColumn(\"sale_date\", to_date(col(\"sale_date\")))\n",
        "#aggregates the cleaned data: groups by sale_date and category, and calculates the total revenue for each combination. calculates the total revenue for each group by multiplying price and quantity and summing them up.\n",
        "df_agg = df_clean.groupBy(\"sale_date\", \"category\").agg(spark_sum(col(\"price\") * col(\"quantity\")).alias(\"revenue\"))\n",
        "\n",
        "# Window function for ranking top categories per day   This section calculates the rank of each category's revenue within each day\n",
        "#defines a window specification. It partitions the data by sale_date (meaning ranks are calculated independently for each day) and orders the results within each partition by revenue in descending order.\n",
        "#applies this window function to df_agg, creating a new column named rank that assigns a rank to each category based on its revenue within its respective day.\n",
        "window_spec = Window.partitionBy(\"sale_date\").orderBy(desc(\"revenue\"))\n",
        "df_ranked = df_agg.withColumn(\"rank\", rank().over(window_spec))\n",
        "#displays the final df_ranked DataFrame, ordered by sale_date and then by revenue in descending order, showing the top-performing categories for each day.\n",
        "\n",
        "df_ranked.orderBy(\"sale_date\", desc(\"revenue\")).show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CnZVdqdpXXWU",
        "outputId": "3e3fc96a-2b51-485b-f1ed-2b5e7dc6c674"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+-----------+-------+----+\n",
            "|sale_date |category   |revenue|rank|\n",
            "+----------+-----------+-------+----+\n",
            "|2025-01-01|Clothing   |250    |1   |\n",
            "|2025-01-01|Electronics|200    |2   |\n",
            "|2025-01-01|Books      |200    |2   |\n",
            "|2025-01-02|Clothing   |150    |1   |\n",
            "|2025-01-02|Electronics|100    |2   |\n",
            "+----------+-----------+-------+----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The complete PySpark stock P/E ratio ETL pipeline demonstrates Spark's distributed computing from extract â†’ transform â†’ load. Each line builds toward parallel execution across a cluster while abstracting complexity from the developer.[1]"
      ],
      "metadata": {
        "id": "GbTsr3ATtoFx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ===== COMPLETE STOCK P/E RATIO ETL PIPELINE =====\n",
        "# Video: https://www.youtube.com/watch?v=KAuIvccwbPY\n",
        "\n",
        "# 1. Initialize SparkSession (Driver Program Entry Point)\n",
        "#: Imports core Spark SQL components. SparkSession creates the driver entry point; col() references DataFrame columns in expressions.\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, when, avg, count\n",
        "\n",
        "# Create SparkSession - connects to cluster (local/4 nodes/serverless)\n",
        "# Use cluster: \"yarn\", \"k8s://...\", or omit for default\n",
        "#Purpose: Creates driver program connecting to cluster.\n",
        "#â€¢\tbuilder: Fluent API for configuration\n",
        "#â€¢\tappName: Identifies job in Spark UI/Cluster Manager\n",
        "#â€¢\tmaster(\"local[*]\"): Uses all CPU cores locally (change to \"yarn\" for cluster)\n",
        "#â€¢\tgetOrCreate(): Singleton pattern - reuses existing session or creates new one. Result: Driver ready to orchestrate workers.\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"StockPEratioDemo\") \\\n",
        "    .master(\"local[*]\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "print(\"âœ… SparkSession created. Cluster ready for distributed processing.\")\n",
        "\n",
        "\n",
        "\n",
        "# ETL Step 1 - Distributed read operation.\n",
        "#â€¢\tspark.read: DataFrameReader for structured formats\n",
        "#â€¢\theader=\"true\": First row becomes column names\n",
        "#â€¢\tinferSchema=\"true\": Auto-detects types (stringâ†’double)\n",
        "#â€¢\tKey: Spark partitions file across executors automatically\n",
        "#Lazy: No data loaded yet - just logical plan created.\n",
        "\n",
        "# 2. EXTRACT: Load CSV (Spark auto-partitions across workers)\n",
        "df_raw = spark.read \\\n",
        "    .option(\"header\", \"true\") \\\n",
        "    .option(\"inferSchema\", \"true\") \\\n",
        "    .csv(\"/content/sample_data/stocks.csv\")  # Scales to HDFS/S3 paths\n",
        "\n",
        "print(\"ðŸ“Š Raw data schema:\")\n",
        "#Purpose: Actions trigger first execution.\n",
        "#â€¢\tprintSchema(): Shows inferred types (double for Price/EPS)\n",
        "#â€¢\tshow(): Materializes top 20 rows to driver console\n",
        "#Triggers: Catalyst optimizer + DAG execution across cluster.\n",
        "\n",
        "df_raw.printSchema()\n",
        "df_raw.show()\n",
        "\n",
        "\n",
        "\n",
        "#Purpose: ETL Step 2 - Business transformations (lazy).\n",
        "#â€¢\tFirst withColumn: Creates pe_ratio = Price Ã· EPS\n",
        "#â€¢\tSecond withColumn: Conditional logic chains (when/otherwise)\n",
        "#â€¢\tImmutable: Each returns new DataFrame\n",
        "#â€¢\tBroadcast: Formulas applied in parallel on each partition\n",
        "#DAG Built: Logical plan grows: read â†’ divide â†’ when() â†’ when()\n",
        "\n",
        "\n",
        "\n",
        "# 3. TRANSFORM: Business logic (lazy evaluation - no execution yet)\n",
        "df_pe = df_raw.withColumn(\n",
        "    \"pe_ratio\",\n",
        "    col(\"Price\") / col(\"EPS\")\n",
        ").withColumn(\n",
        "    \"pe_category\",\n",
        "    when(col(\"pe_ratio\") < 15, \"Undervalued\")\n",
        "    .when(col(\"pe_ratio\") > 30, \"Overvalued\")\n",
        "    .otherwise(\"Fair\")\n",
        ")\n",
        "\n",
        "print(\"ðŸ”„ Transformation DAG built (lazy)\")\n",
        "\n",
        "\n",
        "\n",
        "#Purpose: Triggers full execution pipeline.\n",
        "#â€¢\torderBy: Shuffle stage (data crosses partitions)\n",
        "#â€¢\tselect: Column projection optimization\n",
        "#â€¢\tshow(truncate=False): Full column display\n",
        "#Physical Plan: Driver â†’ DAG Scheduler â†’ Task Scheduler â†’ Executors.\n",
        "\n",
        "\n",
        "# 4. LOAD/ACTION: Triggers execution across cluster\n",
        "print(\"ðŸš€ EXECUTING across cluster...\")\n",
        "df_result = df_pe.orderBy(col(\"pe_ratio\"))\n",
        "df_result.select(\"Symbol\", \"Price\", \"EPS\", \"pe_ratio\", \"pe_category\").show(truncate=False)\n",
        "\n",
        "\n",
        "#Purpose: Real-world ETL aggregation pattern.\n",
        "#â€¢\tgroupBy: Shuffle by pe_category\n",
        "#â€¢\tagg: Multiple aggregations in single pass\n",
        "#â€¢\talias: Renames output columns\n",
        "#Optimization: Catalyst combines with prior operations.\n",
        "\n",
        "\n",
        "# 5. Advanced: Aggregate analytics (real ETL use case)\n",
        "df_summary = df_pe.groupBy(\"pe_category\").agg(\n",
        "    avg(\"pe_ratio\").alias(\"avg_pe\"),\n",
        "    count(\"Symbol\").alias(\"stock_count\")\n",
        ").orderBy(\"pe_category\")\n",
        "print(\"ðŸ“ˆ Portfolio Summary:\")\n",
        "df_summary.show()\n",
        "\n",
        "\n",
        "\n",
        "#Purpose: ETL Step 3 - Persist transformed data.\n",
        "#â€¢\tcoalesce(1): Single output file (remove for partitioned writes)\n",
        "#â€¢\tmode(\"overwrite\"): Replace existing output\n",
        "#â€¢\tDistributed Write: Executors write parallel partitions.\n",
        "\n",
        "# 6. Write results (complete ETL)\n",
        "df_result.coalesce(1).write \\\n",
        "    .mode(\"overwrite\") \\\n",
        "    .option(\"header\", \"true\") \\\n",
        "    .csv(\"stocks_pe_analysis\")\n",
        "\n",
        "print(\"ðŸ’¾ Results written to stocks_pe_analysis/\")\n",
        "\n",
        "\n",
        "#Purpose: Release cluster resources (executors, memory).\n",
        "#Execution Timeline\n",
        "#1. Code written â†’ Logical Plan (lazy transformations)\n",
        "#2. .show()/.write() â†’ Catalyst Optimization â†’ Physical Plan\n",
        "#3. DAG Scheduler â†’ Stages â†’ Tasks â†’ Executors (parallel)\n",
        "#4. Results â†’ Driver â†’ Console/Storage\n",
        "\n",
        "# 7. Cleanup\n",
        "spark.stop()\n",
        "print(\"âœ… Spark cluster shutdown.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CYuCszk7tq8K",
        "outputId": "5dfc1b60-40e8-40bb-c757-1441aa3d9660"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… SparkSession created. Cluster ready for distributed processing.\n",
            "ðŸ“Š Raw data schema:\n",
            "root\n",
            " |-- Symbol: string (nullable = true)\n",
            " |-- Price: double (nullable = true)\n",
            " |-- EPS: double (nullable = true)\n",
            "\n",
            "+------+-------+-----+\n",
            "|Symbol|  Price|  EPS|\n",
            "+------+-------+-----+\n",
            "|  AAPL|  175.5| 6.16|\n",
            "|  GOOG|2850.25|145.6|\n",
            "|  MSFT|  425.8| 11.8|\n",
            "|  TSLA|  245.3| 3.65|\n",
            "|  AMZN|  186.8| 4.12|\n",
            "|  META|  567.9|20.35|\n",
            "|  NVDA| 890.45|12.05|\n",
            "+------+-------+-----+\n",
            "\n",
            "ðŸ”„ Transformation DAG built (lazy)\n",
            "ðŸš€ EXECUTING across cluster...\n",
            "+------+-------+-----+------------------+-----------+\n",
            "|Symbol|Price  |EPS  |pe_ratio          |pe_category|\n",
            "+------+-------+-----+------------------+-----------+\n",
            "|GOOG  |2850.25|145.6|19.575892857142858|Fair       |\n",
            "|META  |567.9  |20.35|27.906633906633903|Fair       |\n",
            "|AAPL  |175.5  |6.16 |28.490259740259738|Fair       |\n",
            "|MSFT  |425.8  |11.8 |36.08474576271186 |Overvalued |\n",
            "|AMZN  |186.8  |4.12 |45.33980582524272 |Overvalued |\n",
            "|TSLA  |245.3  |3.65 |67.2054794520548  |Overvalued |\n",
            "|NVDA  |890.45 |12.05|73.89626556016597 |Overvalued |\n",
            "+------+-------+-----+------------------+-----------+\n",
            "\n",
            "ðŸ“ˆ Portfolio Summary:\n",
            "+-----------+------------------+-----------+\n",
            "|pe_category|            avg_pe|stock_count|\n",
            "+-----------+------------------+-----------+\n",
            "|       Fair|25.324262168012165|          3|\n",
            "| Overvalued| 55.63157415004384|          4|\n",
            "+-----------+------------------+-----------+\n",
            "\n",
            "ðŸ’¾ Results written to stocks_pe_analysis/\n",
            "âœ… Spark cluster shutdown.\n"
          ]
        }
      ]
    }
  ]
}