{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "deeplearningRNN.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMRdo4GBZtGTInTcM7pdXkX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/urmilapol/urmilapolprojects/blob/master/deeplearningRNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t7bxPRyTAlIC"
      },
      "outputs": [],
      "source": [
        "#Import the necessary data science libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "#Import the data set as a pandas DataFrame\n",
        "training_data = pd.read_csv('FB_training_data.csv')\n",
        "\n",
        "#Transform the data set into a NumPy array\n",
        "training_data = training_data.iloc[:, 1].values\n",
        "\n",
        "#Apply feature scaling to the data set\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "scaler = MinMaxScaler()\n",
        "training_data = scaler.fit_transform(training_data.reshape(-1, 1))\n",
        "\n",
        "#Initialize our x_training_data and y_training_data variables \n",
        "#as empty Python lists\n",
        "x_training_data = []\n",
        "y_training_data =[]\n",
        "\n",
        "#Populate the Python lists using 40 timesteps\n",
        "for i in range(40, len(training_data)):\n",
        "    x_training_data.append(training_data[i-40:i, 0])\n",
        "    y_training_data.append(training_data[i, 0])\n",
        "    \n",
        "#Transforming our lists into NumPy arrays\n",
        "x_training_data = np.array(x_training_data)\n",
        "y_training_data = np.array(y_training_data)\n",
        "\n",
        "#Verifying the shape of the NumPy arrays\n",
        "print(x_training_data.shape)\n",
        "print(y_training_data.shape)\n",
        "\n",
        "#Reshaping the NumPy array to meet TensorFlow standards\n",
        "x_training_data = np.reshape(x_training_data, (x_training_data.shape[0], \n",
        "                                               x_training_data.shape[1], \n",
        "                                               1))\n",
        "\n",
        "#Printing the new shape of x_training_data\n",
        "print(x_training_data.shape)\n",
        "\n",
        "#Importing our TensorFlow libraries\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import LSTM\n",
        "from tensorflow.keras.layers import Dropout\n",
        "\n",
        "#Initializing our recurrent neural network\n",
        "rnn = Sequential()\n",
        "\n",
        "#Adding our first LSTM layer\n",
        "rnn.add(LSTM(units = 45, return_sequences = True, input_shape = (x_training_data.shape[1], 1)))\n",
        "\n",
        "#Perform some dropout regularization\n",
        "rnn.add(Dropout(0.2))\n",
        "\n",
        "#Adding three more LSTM layers with dropout regularization\n",
        "for i in [True, True, False]:\n",
        "    rnn.add(LSTM(units = 45, return_sequences = i))\n",
        "    rnn.add(Dropout(0.2))\n",
        "\n",
        "#(Original code for the three additional LSTM layers)\n",
        "# rnn.add(LSTM(units = 45, return_sequences = True))\n",
        "# rnn.add(Dropout(0.2))\n",
        "\n",
        "# rnn.add(LSTM(units = 45, return_sequences = True))\n",
        "# rnn.add(Dropout(0.2))\n",
        "\n",
        "# rnn.add(LSTM(units = 45))\n",
        "# rnn.add(Dropout(0.2))\n",
        "\n",
        "#Adding our output layer\n",
        "rnn.add(Dense(units = 1))\n",
        "\n",
        "#Compiling the recurrent neural network\n",
        "rnn.compile(optimizer = 'adam', loss = 'mean_squared_error')\n",
        "\n",
        "#Training the recurrent neural network\n",
        "rnn.fit(x_training_data, y_training_data, epochs = 100, batch_size = 32)\n",
        "\n",
        "#Import the test data set and transform it into a NumPy array\n",
        "test_data = pd.read_csv('FB_test_data.csv')\n",
        "test_data = test_data.iloc[:, 1].values\n",
        "\n",
        "#Make sure the test data's shape makes sense\n",
        "print(test_data.shape)\n",
        "\n",
        "#Plot the test data\n",
        "plt.plot(test_data)\n",
        "\n",
        "#Create unscaled training data and test data objects\n",
        "unscaled_training_data = pd.read_csv('FB_training_data.csv')\n",
        "unscaled_test_data = pd.read_csv('FB_test_data.csv')\n",
        "\n",
        "#Concatenate the unscaled data\n",
        "all_data = pd.concat((unscaled_x_training_data['Open'], unscaled_test_data['Open']), axis = 0)\n",
        "\n",
        "#Create our x_test_data object, which has each January day + the 40 prior days\n",
        "x_test_data = all_data[len(all_data) - len(test_data) - 40:].values\n",
        "x_test_data = np.reshape(x_test_data, (-1, 1))\n",
        "\n",
        "#Scale the test data\n",
        "x_test_data = scaler.transform(x_test_data)\n",
        "\n",
        "#Grouping our test data\n",
        "final_x_test_data = []\n",
        "for i in range(40, len(x_test_data)):\n",
        "    final_x_test_data.append(x_test_data[i-40:i, 0])\n",
        "final_x_test_data = np.array(final_x_test_data)\n",
        "\n",
        "#Reshaping the NumPy array to meet TensorFlow standards\n",
        "final_x_test_data = np.reshape(final_x_test_data, (final_x_test_data.shape[0], \n",
        "                                               final_x_test_data.shape[1], \n",
        "                                               1))\n",
        "\n",
        "#Generating our predicted values\n",
        "predictions = rnn.predict(final_x_test_data)\n",
        "\n",
        "#Plotting our predicted values\n",
        "plt.clf() #This clears the old plot from our canvas\n",
        "plt.plot(predictions)\n",
        "\n",
        "#Unscaling the predicted values and re-plotting the data\n",
        "unscaled_predictions = scaler.inverse_transform(predictions)\n",
        "plt.clf() #This clears the first prediction plot from our canvas\n",
        "plt.plot(unscaled_predictions)\n",
        "\n",
        "#Plotting the predicted values against Facebook's actual stock price\n",
        "plt.plot(unscaled_predictions, color = '#135485', label = \"Predictions\")\n",
        "plt.plot(test_data, color = 'black', label = \"Real Data\")\n",
        "plt.title('Facebook Stock Price Predictions')"
      ]
    }
  ]
}